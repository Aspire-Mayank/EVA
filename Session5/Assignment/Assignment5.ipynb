{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1_JSsorxuix",
        "colab_type": "text"
      },
      "source": [
        "#Assignment -5 :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ThWCDH-vyUj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "c938b1f5-0208-4071-f91d-d98b4fe2cd4c"
      },
      "source": [
        "#https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXc19Gg8E3SI",
        "colab_type": "text"
      },
      "source": [
        "###Loading Keras and Model related packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7di2gkFyJpq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import regularizers\n",
        "\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFeCOKBTyUrK",
        "colab_type": "text"
      },
      "source": [
        "###Loading preshuffled MNIST Dataset in Train and Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPR9q8YnyNlP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "fdd9ed77-56fc-4a13-8bdd-63a604bed699"
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pB7IlPOryerD",
        "colab_type": "text"
      },
      "source": [
        "###Visualization of MNIST Dataset which has sample of 60k images size of 28x28"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICIGepqvyfXd",
        "colab_type": "code",
        "outputId": "fd8a1c91-81ce-437a-faeb-c30582ed87a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[9])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f295492ed30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlkRN4JIGcrAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWe1JCCEFuAY",
        "colab_type": "text"
      },
      "source": [
        "###Preprocess input data for Keras.\n",
        ">***A full-color image with all 3 RGB channels will have a depth of 3.Our MNIST images only have a depth of 1, but we must explicitly declare that.***\n",
        "\n",
        ">***In other words, we want to transform our dataset from having shape (n, width, height) to (n, depth, width, height)***."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJf--rfYyoUw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Converting/Reshaping 2-D dimenional Grayscale image to keras specific 4-D input/output form\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5rYjuNkytML",
        "colab_type": "text"
      },
      "source": [
        "###Let's look at shape of our class label data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xdi_Tgtzytli",
        "colab_type": "code",
        "outputId": "a6c18773-c6fb-4074-b71d-61315b826473",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print(y_train.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwhwiw22G3ug",
        "colab_type": "text"
      },
      "source": [
        "###We should have 10 different classes, one for each digit, one, but it's looks like we have 1D array.\n",
        "###Let's take a look at labels of first 10 traning samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OU5HEUpIy6JA",
        "colab_type": "code",
        "outputId": "934a48bd-3619-4c62-b7a8-4e487204c9d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "y_train[:10]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOTFtqagy_Au",
        "colab_type": "text"
      },
      "source": [
        "###And there's the problem. The y_train and y_test data are not split into 10 distinct class labels, but rather are represented as a single array with the class values.\n",
        "\n",
        ">***1. We can fix this easily:***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-DC7lOiy_Vw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xb5G-5AeQeZE",
        "colab_type": "code",
        "outputId": "1713e628-91ce-4c3a-a0aa-28a9a9efea03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print('Train min=%.3f, max=%.3f' % (X_train.min(), X_train.max()))\n",
        "print('Test min=%.3f, max=%.3f' % (X_test.min(), X_test.max()))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train min=0.000, max=255.000\n",
            "Test min=0.000, max=255.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH58QMAOIKVe",
        "colab_type": "text"
      },
      "source": [
        "###Now have a look, we have 10 different classes for individual for each digit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaOaNePvzD_5",
        "colab_type": "code",
        "outputId": "6e32c60b-393d-4d1a-d95d-d207a33f4802",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print (Y_train.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5djunZ5yzJ6i",
        "colab_type": "code",
        "outputId": "7469289d-fbc5-4330-e040-999fc63a9cae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        }
      },
      "source": [
        "Y_train[:10]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW4lTeMhzL9w",
        "colab_type": "text"
      },
      "source": [
        "###Image Normalization\n",
        ">***1. Image standarization technique*** : Standardization is a data scaling technique that assumes that the distribution of the data is Gaussian and shifts the distribution of the data to have a mean of zero and a standard deviation of one.\n",
        "\n",
        ">***2. Standarization with Mean of pixel*** : Standardization of images is achieved by subtracting the mean pixel value and dividing the result by the standard deviation of the pixel values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Blm9ALx7RPeh",
        "colab_type": "code",
        "outputId": "221f9041-2e89-4afc-8c9d-4dbde1163366",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# create generator to standardize images\n",
        "datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
        "\n",
        "# calculate mean on training dataset\n",
        "datagen.fit(X_train)\n",
        "\n",
        "# prepare an iterators to scale images\n",
        "train_iterator = datagen.flow(X_train, Y_train, batch_size=64)\n",
        "test_iterator = datagen.flow(X_test, Y_test, batch_size=64)\n",
        "print('Batches train=%d, test=%d' % (len(train_iterator), len(test_iterator)))\n",
        "\n",
        "# confirm the scaling works\n",
        "batchX, batchy = train_iterator.next()\n",
        "print('Batch shape=%s, min=%.3f, max=%.3f' % (batchX.shape, batchX.min(), batchX.max()))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batches train=938, test=157\n",
            "Batch shape=(64, 28, 28, 1), min=-0.424, max=2.822\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdVNBy78VNbt",
        "colab_type": "text"
      },
      "source": [
        "##Define Model Architechture :\n",
        "\n",
        " >***BatchNormalization*** : Helps to normalize features, weights and initialization of network.\n",
        " \n",
        " >***Kernal Regularization (L2)*** : penalization of weights in convolution layers which helps to reduce loss and Learning rate, overall model perform better with reduced loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "088b9cbd-d013-43b4-dd1d-dae41909a01f",
        "id": "gHHx43b2HSWl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "#Importing core Functions of Keras layers which will use in Neural Network\n",
        "from keras.layers import Activation, BatchNormalization\n",
        "\n",
        "#Now Will start with Sequential Model\n",
        "model = Sequential()\n",
        "\n",
        "#Adding Input Layer:\n",
        "#Input Image size 28x28x1\n",
        "#convolution filter/kernal size is 3x3x1 of count/row : 10\n",
        "#Adding L2 Regularizers to penalize weights to 10% in layer wise.\n",
        "model.add(Convolution2D(10, 3, 3, kernel_regularizer=regularizers.l2(0.01), activation='relu', input_shape=(28,28,1))) # 26\n",
        "#BatchNorms will normalize weights using Mean, Variance of each channels pixel-wise in particular layer\n",
        "model.add(BatchNormalization())\n",
        "#adding Dropouts(10% drops in pixel values randomly) which drops random nueron/pixel in particular layer which helps model to learn other params exceplicitly\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "#Input image to these layer is 26x26x10\n",
        "#Kernal/filter size : 3x3x10 of number/count/row : 16\n",
        "#Adding L2 Regularizers to penalize weights to 10% in layer wise.\n",
        "model.add(Convolution2D(16, 3, 3, kernel_regularizer=regularizers.l2(0.01), activation='relu')) # 24\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "#Input image to these layer is 24x24x10\n",
        "#Kernal/filter size : 1x1x16 of number/count/row : 10\n",
        "#1x1 convolution layer which helps to reduce channel size and increase particular features amplitude.\n",
        "model.add(Convolution2D(10, 1, 1, kernel_regularizer=regularizers.l2(0.01), activation='relu')) # 24\n",
        "\n",
        "#MaxPooling2D is a way to reduce the number of parameters in our model by sliding a 2x2 pooling,\n",
        "#filter across the previous layer and taking the max of the 4 values in the 2x2 filter.\n",
        "model.add(MaxPooling2D(pool_size=(2, 2))) # 12\n",
        "#model.add(Convolution2D(10, 1, 1, activation='relu')) # 12\n",
        "\n",
        "#Input image to these layer is 12x12x10\n",
        "#Kernal/filter size : 3x3x10 of number/count/row : 16\n",
        "model.add(Convolution2D(16, 3, 3, kernel_regularizer=regularizers.l2(0.01), activation='relu')) # 10\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "#Input image to these layer is 10x10x16\n",
        "#Kernal/filter size : 3x3x16 of number/count/row : 16\n",
        "model.add(Convolution2D(16, 3, 3, kernel_regularizer=regularizers.l2(0.01), activation='relu')) # 8\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "#Input image to these layer is 8x8x16\n",
        "#Kernal/filter size : 3x3x16 of number/count/row : 16\n",
        "model.add(Convolution2D(16, 3, 3, kernel_regularizer=regularizers.l2(0.01), activation='relu')) # 6\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "#Input image to these layer is 6x6x16\n",
        "#Kernal/filter size : 3x3x16 of number/count/row : 16\n",
        "model.add(Convolution2D(16, 3, 3, kernel_regularizer=regularizers.l2(0.01), activation='relu')) # 4\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "#model.add(Convolution2D(10, 1, activation='relu')) #4\n",
        "model.add(Convolution2D(10, 4, 4))\n",
        "\n",
        "#Before prediction layer we should flatten all convolution weights of neuron\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), kernel_regularizer=<keras.reg..., activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), kernel_regularizer=<keras.reg..., activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (1, 1), kernel_regularizer=<keras.reg..., activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), kernel_regularizer=<keras.reg..., activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), kernel_regularizer=<keras.reg..., activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), kernel_regularizer=<keras.reg..., activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:53: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), kernel_regularizer=<keras.reg..., activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:58: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (4, 4))`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFyOTSzUzcoL",
        "colab_type": "code",
        "outputId": "1ee8298e-839c-4f7c-c54f-08e9d4b68452",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 903
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_17 (Conv2D)           (None, 26, 26, 10)        100       \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 26, 26, 10)        40        \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 26, 26, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 24, 24, 16)        1456      \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 24, 24, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 24, 24, 10)        170       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 12, 12, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 10, 10, 16)        1456      \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 10, 10, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 10, 10, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 8, 8, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 8, 8, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 6, 6, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 6, 6, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 6, 6, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 4, 4, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 4, 4, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 1, 1, 10)          2570      \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 13,072\n",
            "Trainable params: 12,892\n",
            "Non-trainable params: 180\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTNZOoydziFN",
        "colab_type": "text"
      },
      "source": [
        "###Model Compilation:\n",
        ">***1. we have LR scheduler*** : It changes or reduce LR with epochs increases \n",
        "\n",
        ">***2. we have callbacks*** : It will use to save best model as per epochs with reduced validation loss.\n",
        "\n",
        ">***3. fit_Generator*** : It has Train and test iterator which helps to scale images, how epochs will go step by step and validation steps on validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pl1acS34zicp",
        "colab_type": "code",
        "outputId": "eb998332-7963-4508-c82a-b4ade6b61697",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2710
        }
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
        "def scheduler(epoch, lr):\n",
        "  return round(0.003 * 1/(1 + 0.300 * epoch), 10)\n",
        "\n",
        "# load weights\n",
        "#model.load_weights(\"weights.best.hdf5\")\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])\n",
        "\n",
        "#model.fit(X_train, Y_train, batch_size=64, nb_epoch=20, verbose=1, validation_data=(X_test,Y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])\n",
        "\n",
        "checkpoint=ModelCheckpoint(filepath=\"weights.best.model.hdf5\",\n",
        "                           monitor='val_loss',\n",
        "                           verbose=0, save_best_only=True,\n",
        "                           save_weights_only=False,\n",
        "                           mode='max', period=1)\n",
        "\n",
        "# fit model with generator\n",
        "model.fit_generator(train_iterator,\n",
        "                    steps_per_epoch=256,\n",
        "                    epochs=40,\n",
        "                    callbacks=[checkpoint, LearningRateScheduler(scheduler, verbose=1)],\n",
        "                    validation_data=test_iterator,\n",
        "                    validation_steps=512)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "256/256 [==============================] - 72s 282ms/step - loss: 0.6257 - acc: 0.9366 - val_loss: 0.5282 - val_acc: 0.8950\n",
            "Epoch 2/40\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0023076923.\n",
            "256/256 [==============================] - 68s 267ms/step - loss: 0.2853 - acc: 0.9622 - val_loss: 0.4181 - val_acc: 0.9049\n",
            "Epoch 3/40\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.001875.\n",
            "256/256 [==============================] - 68s 267ms/step - loss: 0.2081 - acc: 0.9709 - val_loss: 0.2170 - val_acc: 0.9631\n",
            "Epoch 4/40\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015789474.\n",
            "256/256 [==============================] - 69s 269ms/step - loss: 0.1744 - acc: 0.9730 - val_loss: 0.2475 - val_acc: 0.9454\n",
            "Epoch 5/40\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013636364.\n",
            "256/256 [==============================] - 68s 267ms/step - loss: 0.1686 - acc: 0.9713 - val_loss: 0.1265 - val_acc: 0.9840\n",
            "Epoch 6/40\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0012.\n",
            "256/256 [==============================] - 69s 269ms/step - loss: 0.1339 - acc: 0.9792 - val_loss: 0.1264 - val_acc: 0.9817\n",
            "Epoch 7/40\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010714286.\n",
            "256/256 [==============================] - 70s 274ms/step - loss: 0.1353 - acc: 0.9769 - val_loss: 0.1242 - val_acc: 0.9814\n",
            "Epoch 8/40\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009677419.\n",
            "256/256 [==============================] - 70s 274ms/step - loss: 0.1307 - acc: 0.9772 - val_loss: 0.1056 - val_acc: 0.9847\n",
            "Epoch 9/40\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008823529.\n",
            "256/256 [==============================] - 70s 272ms/step - loss: 0.1120 - acc: 0.9811 - val_loss: 0.0943 - val_acc: 0.9861\n",
            "Epoch 10/40\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0008108108.\n",
            "256/256 [==============================] - 69s 271ms/step - loss: 0.1079 - acc: 0.9819 - val_loss: 0.0966 - val_acc: 0.9857\n",
            "Epoch 11/40\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.00075.\n",
            "256/256 [==============================] - 68s 267ms/step - loss: 0.1031 - acc: 0.9815 - val_loss: 0.0919 - val_acc: 0.9862\n",
            "Epoch 12/40\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.0006976744.\n",
            "256/256 [==============================] - 70s 275ms/step - loss: 0.0964 - acc: 0.9832 - val_loss: 0.0841 - val_acc: 0.9868\n",
            "Epoch 13/40\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006521739.\n",
            "256/256 [==============================] - 69s 270ms/step - loss: 0.0967 - acc: 0.9829 - val_loss: 0.0766 - val_acc: 0.9892\n",
            "Epoch 14/40\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0006122449.\n",
            "256/256 [==============================] - 70s 272ms/step - loss: 0.0930 - acc: 0.9841 - val_loss: 0.0843 - val_acc: 0.9864\n",
            "Epoch 15/40\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005769231.\n",
            "256/256 [==============================] - 70s 272ms/step - loss: 0.0887 - acc: 0.9849 - val_loss: 0.0857 - val_acc: 0.9847\n",
            "Epoch 16/40\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005454545.\n",
            "256/256 [==============================] - 68s 266ms/step - loss: 0.0881 - acc: 0.9845 - val_loss: 0.0762 - val_acc: 0.9875\n",
            "Epoch 17/40\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.0005172414.\n",
            "256/256 [==============================] - 69s 268ms/step - loss: 0.0795 - acc: 0.9872 - val_loss: 0.0738 - val_acc: 0.9877\n",
            "Epoch 18/40\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004918033.\n",
            "256/256 [==============================] - 70s 272ms/step - loss: 0.0823 - acc: 0.9846 - val_loss: 0.0704 - val_acc: 0.9885\n",
            "Epoch 19/40\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.00046875.\n",
            "256/256 [==============================] - 70s 273ms/step - loss: 0.0782 - acc: 0.9852 - val_loss: 0.0708 - val_acc: 0.9889\n",
            "Epoch 20/40\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.0004477612.\n",
            "256/256 [==============================] - 70s 273ms/step - loss: 0.0825 - acc: 0.9850 - val_loss: 0.0727 - val_acc: 0.9874\n",
            "Epoch 21/40\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0004285714.\n",
            "256/256 [==============================] - 71s 276ms/step - loss: 0.0750 - acc: 0.9870 - val_loss: 0.0655 - val_acc: 0.9894\n",
            "Epoch 22/40\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.0004109589.\n",
            "256/256 [==============================] - 70s 272ms/step - loss: 0.0796 - acc: 0.9855 - val_loss: 0.0621 - val_acc: 0.9910\n",
            "Epoch 23/40\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0003947368.\n",
            "256/256 [==============================] - 69s 268ms/step - loss: 0.0681 - acc: 0.9891 - val_loss: 0.0566 - val_acc: 0.9916\n",
            "Epoch 24/40\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0003797468.\n",
            "256/256 [==============================] - 68s 267ms/step - loss: 0.0714 - acc: 0.9872 - val_loss: 0.0795 - val_acc: 0.9852\n",
            "Epoch 25/40\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0003658537.\n",
            "256/256 [==============================] - 68s 266ms/step - loss: 0.0634 - acc: 0.9886 - val_loss: 0.0579 - val_acc: 0.9903\n",
            "Epoch 26/40\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0003529412.\n",
            "256/256 [==============================] - 68s 264ms/step - loss: 0.0688 - acc: 0.9877 - val_loss: 0.0562 - val_acc: 0.9894\n",
            "Epoch 27/40\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0003409091.\n",
            "256/256 [==============================] - 68s 264ms/step - loss: 0.0673 - acc: 0.9872 - val_loss: 0.0612 - val_acc: 0.9886\n",
            "Epoch 28/40\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0003296703.\n",
            "256/256 [==============================] - 68s 264ms/step - loss: 0.0638 - acc: 0.9878 - val_loss: 0.0508 - val_acc: 0.9918\n",
            "Epoch 29/40\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.0003191489.\n",
            "256/256 [==============================] - 67s 262ms/step - loss: 0.0604 - acc: 0.9889 - val_loss: 0.0519 - val_acc: 0.9917\n",
            "Epoch 30/40\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0003092784.\n",
            "256/256 [==============================] - 68s 266ms/step - loss: 0.0628 - acc: 0.9878 - val_loss: 0.0585 - val_acc: 0.9886\n",
            "Epoch 31/40\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.0003.\n",
            "256/256 [==============================] - 61s 238ms/step - loss: 0.0622 - acc: 0.9878 - val_loss: 0.0544 - val_acc: 0.9906\n",
            "Epoch 32/40\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.0002912621.\n",
            "256/256 [==============================] - 65s 255ms/step - loss: 0.0581 - acc: 0.9899 - val_loss: 0.0518 - val_acc: 0.9925\n",
            "Epoch 33/40\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.0002830189.\n",
            "256/256 [==============================] - 69s 269ms/step - loss: 0.0571 - acc: 0.9894 - val_loss: 0.0450 - val_acc: 0.9931\n",
            "Epoch 34/40\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.0002752294.\n",
            "256/256 [==============================] - 70s 273ms/step - loss: 0.0587 - acc: 0.9889 - val_loss: 0.0482 - val_acc: 0.9930\n",
            "Epoch 35/40\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.0002678571.\n",
            "256/256 [==============================] - 71s 277ms/step - loss: 0.0584 - acc: 0.9889 - val_loss: 0.0507 - val_acc: 0.9915\n",
            "Epoch 36/40\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.0002608696.\n",
            "256/256 [==============================] - 70s 275ms/step - loss: 0.0572 - acc: 0.9893 - val_loss: 0.0456 - val_acc: 0.9930\n",
            "Epoch 37/40\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.0002542373.\n",
            "256/256 [==============================] - 70s 274ms/step - loss: 0.0548 - acc: 0.9904 - val_loss: 0.0430 - val_acc: 0.9939\n",
            "Epoch 38/40\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.0002479339.\n",
            "256/256 [==============================] - 70s 275ms/step - loss: 0.0527 - acc: 0.9905 - val_loss: 0.0503 - val_acc: 0.9916\n",
            "Epoch 39/40\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.0002419355.\n",
            "256/256 [==============================] - 71s 276ms/step - loss: 0.0487 - acc: 0.9919 - val_loss: 0.0461 - val_acc: 0.9917\n",
            "Epoch 40/40\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.0002362205.\n",
            "256/256 [==============================] - 69s 270ms/step - loss: 0.0528 - acc: 0.9908 - val_loss: 0.0507 - val_acc: 0.9915\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f294b2bd7f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfSGTKSSznqg",
        "colab_type": "text"
      },
      "source": [
        "###Time to evaluate Model with Test Iterator and steps as length of iterator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JRb0p-2zoE2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "score = model.evaluate_generator(test_iterator, steps=len(test_iterator), verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KztNs8PoRMqS",
        "colab_type": "text"
      },
      "source": [
        "###Score to achieve ***99.39*** validation accurarcy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9DLMnNRzrTQ",
        "colab_type": "code",
        "outputId": "da75155e-076d-422c-a512-33135a1d122d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print(score)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.05082696515023708, 0.991]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6Vt6gxfzuPg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdUWu4Lrz5xS",
        "colab_type": "code",
        "outputId": "5f39ea5e-4a14-4436-dbef-b5409dd143b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "source": [
        "print(y_pred[:9])\n",
        "print(y_test[:9])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0000000e+00 0.0000000e+00 8.9109215e-25 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
            " [0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
            " [0.0000000e+00 1.3156852e-13 4.5843218e-03 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00 9.9540210e-01 0.0000000e+00 1.3541966e-05 0.0000000e+00]\n",
            " [4.1242116e-36 0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00 2.0838800e-35 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
            " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00\n",
            "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
            " [0.0000000e+00 1.3046669e-14 5.9961622e-09 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00 4.6261773e-03 3.9503586e-33 9.9537379e-01 0.0000000e+00]\n",
            " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 9.9847037e-01\n",
            "  0.0000000e+00 0.0000000e+00 1.2665432e-18 6.9533463e-23 1.5296320e-03]\n",
            " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.4875440e-28\n",
            "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00]\n",
            " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
            "  1.0000000e+00 7.0297956e-37 0.0000000e+00 0.0000000e+00 0.0000000e+00]]\n",
            "[7 2 1 0 4 1 4 9 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE-WlDNhRkvV",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dM8P1Nz0Eys",
        "colab_type": "code",
        "outputId": "74941387-389f-41e3-8b1a-3421b57d8e70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1020
        }
      },
      "source": [
        "import pandas as pd\n",
        "d={'pred': model.predict_classes(X_test),\n",
        "   'true': np.argmax(Y_test, axis=1)}\n",
        "df=pd.DataFrame(data=d)\n",
        "\n",
        "array1 = np.array(df[(df.pred!=df.true) & (df.true==1)].index)\n",
        "print(array1)\n",
        "\n",
        "df2 =df[(df.pred!=df.true)]\n",
        "df2.shape"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   2    5   39   40   57   89   94  107  135  137  143  176  190  196\n",
            "  202  204  224  228  276  279  288  302  348  354  377  378  388  393\n",
            "  416  430  455  476  480  489  506  523  537  615  619  672  682  695\n",
            "  716  725  745  749  754  767  790  818  824  835  836  840  848  850\n",
            "  900  907  956  964  967  977 1011 1019 1025 1027 1030 1037 1038 1040\n",
            " 1054 1097 1129 1136 1139 1189 1211 1214 1240 1257 1280 1295 1305 1316\n",
            " 1318 1329 1351 1358 1392 1403 1424 1448 1515 1528 1555 1582 1630 1633\n",
            " 1643 1646 1657 1659 1691 1704 1715 1729 1760 1766 1773 1780 1791 1820\n",
            " 1830 1836 1838 1868 1884 1885 1897 1900 1909 1922 1945 1988 1993 1994\n",
            " 2018 2027 2041 2051 2137 2154 2182 2221 2228 2235 2239 2245 2258 2261\n",
            " 2273 2276 2277 2283 2302 2315 2316 2343 2355 2357 2358 2359 2366 2379\n",
            " 2398 2409 2411 2416 2418 2434 2473 2510 2524 2529 2541 2553 2576 2590\n",
            " 2599 2626 2655 2661 2676 2693 2704 2705 2706 2719 2725 2734 2746 2753\n",
            " 2786 2788 2789 2816 2825 2827 2867 2878 2880 2885 2912 2928 2943 2946\n",
            " 2950 2965 2974 2982 2997 3014 3019 3026 3027 3050 3073 3092 3097 3099\n",
            " 3126 3132 3148 3196 3203 3211 3214 3227 3244 3253 3255 3264 3268 3272\n",
            " 3276 3314 3320 3351 3380 3386 3419 3420 3421 3433 3434 3438 3452 3457\n",
            " 3471 3480 3532 3546 3562 3593 3601 3605 3606 3625 3638 3649 3652 3679\n",
            " 3689 3699 3733 3761 3789 3809 3815 3843 3851 3852 3858 3894 3906 3922\n",
            " 3925 3974 3983 3990 4005 4010 4013 4021 4032 4039 4104 4105 4110 4138\n",
            " 4147 4168 4169 4171 4178 4185 4191 4201 4212 4216 4232 4249 4262 4267\n",
            " 4292 4303 4304 4308 4318 4328 4349 4386 4409 4428 4507 4524 4533 4563\n",
            " 4564 4574 4580 4589 4597 4602 4606 4643 4653 4670 4674 4676 4687 4691\n",
            " 4708 4717 4732 4733 4754 4759 4764 4853 4858 4859 4864 4869 4871 4872\n",
            " 4917 4924 4927 4932 4951 4953 4972 5005 5013 5014 5025 5036 5044 5052\n",
            " 5072 5081 5090 5091 5092 5093 5112 5113 5128 5132 5148 5156 5166 5186\n",
            " 5193 5203 5208 5211 5227 5232 5252 5254 5258 5271 5281 5291 5314 5316\n",
            " 5318 5323 5327 5331 5350 5361 5370 5384 5399 5406 5416 5431 5442 5453\n",
            " 5457 5460 5461 5477 5486 5490 5499 5534 5566 5588 5590 5642 5646 5661\n",
            " 5666 5722 5732 5738 5746 5759 5762 5776 5783 5786 5794 5809 5811 5831\n",
            " 5839 5848 5872 5873 5902 6063 6073 6083 6092 6101 6108 6110 6115 6135\n",
            " 6141 6159 6162 6182 6192 6202 6222 6224 6231 6232 6233 6239 6254 6276\n",
            " 6278 6308 6310 6320 6329 6356 6374 6375 6397 6407 6419 6430 6438 6445\n",
            " 6456 6468 6472 6482 6484 6502 6506 6515 6527 6533 6545 6556 6572 6601\n",
            " 6604 6618 6622 6623 6628 6634 6644 6652 6661 6670 6673 6678 6680 6683\n",
            " 6688 6694 6702 6712 6720 6726 6729 6733 6743 6748 6751 6757 6758 6783\n",
            " 6789 6799 6809 6819 6829 6848 6868 6869 6883 6902 6928 6938 6948 6961\n",
            " 6969 6976 7000 7005 7014 7019 7048 7053 7063 7073 7087 7091 7123 7138\n",
            " 7150 7165 7167 7171 7173 7176 7183 7191 7217 7223 7226 7228 7232 7247\n",
            " 7253 7262 7270 7290 7299 7303 7319 7323 7325 7340 7344 7353 7373 7395\n",
            " 7399 7405 7411 7418 7422 7424 7442 7447 7453 7463 7468 7480 7507 7517\n",
            " 7527 7556 7561 7563 7567 7581 7582 7591 7600 7658 7661 7665 7722 7725\n",
            " 7728 7738 7748 7783 7784 7790 7802 7806 7811 7812 7822 7832 7839 7847\n",
            " 7856 7869 7881 7890 7896 7899 7900 7911 7916 7920 7928 7934 7944 7954\n",
            " 7969 7973 7980 7990 8004 8005 8020 8022 8045 8048 8058 8068 8078 8088\n",
            " 8090 8099 8100 8113 8118 8128 8138 8159 8164 8166 8187 8219 8229 8231\n",
            " 8233 8239 8244 8252 8261 8268 8286 8289 8303 8305 8306 8324 8340 8344\n",
            " 8352 8360 8367 8376 8390 8396 8418 8420 8427 8432 8439 8443 8459 8469\n",
            " 8488 8491 8493 8509 8526 8549 8559 8575 8586 8600 8604 8636 8650 8658\n",
            " 8659 8672 8682 8692 8708 8715 8720 8724 8729 8740 8745 8758 8768 8776\n",
            " 8799 8809 8819 8850 8862 8873 8875 8887 8892 8905 8914 8922 8930 8938\n",
            " 8971 8976 8979 8980 8986 8988 8995 9002 9003 9005 9017 9025 9030 9039\n",
            " 9049 9051 9061 9071 9081 9096 9112 9120 9121 9124 9140 9143 9155 9171\n",
            " 9180 9190 9200 9222 9233 9241 9249 9258 9282 9291 9295 9301 9313 9324\n",
            " 9345 9348 9356 9388 9438 9464 9485 9489 9509 9518 9520 9540 9549 9556\n",
            " 9570 9591 9593 9594 9599 9602 9612 9622 9641 9643 9645 9649 9653 9657\n",
            " 9661 9674 9682 9689 9699 9705 9715 9725 9737 9760 9772 9774 9795 9796\n",
            " 9799 9802 9810 9819 9836 9838 9844 9845 9846 9848 9876 9878 9896 9898\n",
            " 9903 9912 9923 9931 9946 9950 9955 9956 9969 9978 9994]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2385, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    }
  ]
}