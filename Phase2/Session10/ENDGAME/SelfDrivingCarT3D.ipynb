{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SelfDrivingCarT3D.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aspire-Mayank/EVA/blob/master/Phase2/Session10/ENDGAME/SelfDrivingCarT3D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIKSm5DiTOIC",
        "colab_type": "text"
      },
      "source": [
        "# Implementation Approch\n",
        "\n",
        ">Following is the strategy to train\n",
        "  * Load the sand image and move the coordinates (Vector) in env.Step() for simulating car movement.\n",
        "  * State is captured by cropping a portion of sand image from car's position. And then rotating it in the direction of the car in such a way that car's orientation is horizontal i.e 0 degrees from x-axis. This state is passed to Actor network\n",
        "  * Action is 1 dimensional, with its value being amount of angle the car should rotate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oYx4zyISnEY",
        "colab_type": "text"
      },
      "source": [
        "* Following above approch, as we can't run Kivy on Colab and because we don't have GPUs on our desktop, "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WXu1r8qvSzWf"
      },
      "source": [
        "This is Complete course for main understanding from scratch [awesome Deep Reinforcement Learning 2.0 Course on Udemy](https://www.udemy.com/course/deep-reinforcement-learning/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xjm2onHdT-Av"
      },
      "source": [
        "## Importing the libraries\n",
        "* Although Kivy is not used in Colab, we are installing it to use its `Vector` module for simulating Kivy environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1oeheFtR9ZW",
        "colab_type": "code",
        "outputId": "3a2ca14f-418e-410f-f42b-9948b2803266",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "!pip install kivy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kivy in /usr/local/lib/python3.6/dist-packages (1.11.1)\n",
            "Requirement already satisfied: docutils in /usr/local/lib/python3.6/dist-packages (from kivy) (0.15.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from kivy) (2.1.3)\n",
            "Requirement already satisfied: Kivy-Garden>=0.1.4 in /usr/local/lib/python3.6/dist-packages (from kivy) (0.1.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from Kivy-Garden>=0.1.4->kivy) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->Kivy-Garden>=0.1.4->kivy) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->Kivy-Garden>=0.1.4->kivy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->Kivy-Garden>=0.1.4->kivy) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->Kivy-Garden>=0.1.4->kivy) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ikr2p0Js8iB4",
        "colab": {}
      },
      "source": [
        "%matplotlib notebook\n",
        "\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib.animation as animation\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from collections import deque\n",
        "from PIL import Image as PILImage"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y2nGdtlKVydr"
      },
      "source": [
        "# Define Replay Buffer Memory\n",
        "* This is a fixed size array storing multiple experiences.\n",
        "* An experience (aka transition) is defined by the following:\n",
        "  * s: current state in which the agent is\n",
        "  * a: action the agent takes to go to next state\n",
        "  * s': new state agent reaches after taking an action (a)\n",
        "  * r: reward an agent receive for going from state (s) to state (s') by taking action (a)\n",
        "* Initially, agent plays with the environment randomly and fills in replay memory.\n",
        "* Then during training, a batch of experiences is sampled randomly to train the agent.\n",
        "* Also this memory is simultaneously filled as and when agent explores the environment.\n",
        "* If memory is full, then first entry is removed and new entry is added.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u5rW0IDB8nTO",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self, max_size=1e6):\n",
        "    self.storage = []\n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0\n",
        "\n",
        "  def add(self, transition):\n",
        "    if len(self.storage) == self.max_size:\n",
        "      self.storage[int(self.ptr)] = transition\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "    else:\n",
        "      self.storage.append(transition)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
        "    for i in ind: \n",
        "      state, next_state, action, reward, done = self.storage[i]\n",
        "      batch_states.append(np.array(state, copy=False))\n",
        "      batch_next_states.append(np.array(next_state.cpu(), copy=False))\n",
        "      batch_actions.append(np.array(action, copy=False))\n",
        "      batch_rewards.append(np.array(reward, copy=False))\n",
        "      batch_dones.append(np.array(done, copy=False))\n",
        "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyOeU_rR0ccK",
        "colab_type": "text"
      },
      "source": [
        "* Fetch device information. This is very useful to speed up network training/inference by using GPU when available. If GPU is not available then, CPU is used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppZ7G3DACBPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Jb7TTaHxWbQD"
      },
      "source": [
        "## Network Architecture\n",
        "* We build one neural network for the Actor model and one neural network for the Actor target\n",
        "* MobileNet were used for this implementation as very simple CNN network required and also the network should have good speed\n",
        "* Below also hosts common functions used to build Actor and Critic networks\n",
        "* conv_s: does simple conv2d with kernel size 3x3 and padding 1\n",
        "* conv_dw: does depthwise convolution with padding 1\n",
        "* state_model: common state network used for both Actor and Critic networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IWcFnjUhk_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv_s(inp, oup, stride):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "def conv_dw(inp, oup, stride):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n",
        "        nn.BatchNorm2d(inp),\n",
        "        nn.ReLU(inplace=True),\n",
        "\n",
        "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        nn.ReLU(inplace=True),\n",
        "    )\n",
        "\n",
        "def state_model():\n",
        "    return nn.Sequential(\n",
        "            # CHW: 1, 32, 32\n",
        "            conv_s(  1,  32, 1), # 32, 32, 32\n",
        "            conv_dw( 32,  32, 1), # 32, 32, 32\n",
        "            conv_dw( 32,  16, 2), # 32, 16, 16\n",
        "            conv_dw( 16,  16, 1), # 16, 16, 16\n",
        "            conv_dw( 16,  16, 2), # 16,  8,  8\n",
        "            nn.AvgPool2d(8),\n",
        "        )\n",
        "\n",
        "# Creating the architecture of the Neural Network\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.model = state_model()\n",
        "        self.fc1 = nn.Linear(16, 100)\n",
        "        self.fc2 = nn.Linear(100, action_dim)\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        x = x.view(-1, 16)\n",
        "        x = self.fc2(self.fc1(x))\n",
        "        x = self.max_action * torch.tanh(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0CujcdSJkAL",
        "colab_type": "text"
      },
      "source": [
        "### Was trying Basic Network for Actor and Critic but geting error while Transformation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJYfnba8vNU3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class Actor(nn.Module):\n",
        "#     def __init__(self, state_dim, action_dim, max_action):\n",
        "#         super(Actor, self).__init__()\n",
        "#         # Input Block\n",
        "#         self.convblock1 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=state_dim, out_channels=8, kernel_size=(3, 3), padding=(1, 1), bias=False),\n",
        "#             nn.ReLU(),\n",
        "#             nn.BatchNorm2d(8),\n",
        "#             nn.Dropout(0.1))  # output_size = 32\n",
        "\n",
        "#         # CONVOLUTION BLOCK 1\n",
        "#         self.convblock2 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), padding=(1, 1), bias=False),\n",
        "#             nn.ReLU(),\n",
        "#             nn.BatchNorm2d(16),\n",
        "#             nn.Dropout(0.1))  # output_size = 32\n",
        "\n",
        "#         # TRANSITION BLOCK 1\n",
        "#         self.convblock3 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), stride=2, padding=0, bias=False),\n",
        "#             nn.ReLU(),\n",
        "#             nn.BatchNorm2d(16),\n",
        "#             nn.Dropout(0.1))  # output 16\n",
        "\n",
        "#         self.convblock4 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=(1, 1), bias=False),\n",
        "#             nn.ReLU(),\n",
        "#             nn.BatchNorm2d(16),\n",
        "#             nn.Dropout(0.1))  # output 16\n",
        "\n",
        "#         self.convblock5 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=(1, 1), bias=False),\n",
        "#             nn.ReLU(),\n",
        "#             nn.BatchNorm2d(16),\n",
        "#             nn.Dropout(0.1))  # output 16\n",
        "\n",
        "#         self.convblock6 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), stride=2, bias=False),\n",
        "#             nn.ReLU(),\n",
        "#             nn.BatchNorm2d(16),\n",
        "#             nn.Dropout(0.1))  # output 8\n",
        "\n",
        "#         self.convblock7 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=(1, 1), bias=False),\n",
        "#             nn.ReLU(),\n",
        "#             nn.BatchNorm2d(16),\n",
        "#             nn.Dropout(0.1))  # output 8\n",
        "\n",
        "#         self.convblock8 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=(1, 1), bias=False),\n",
        "#             nn.ReLU(),\n",
        "#             nn.BatchNorm2d(16),\n",
        "#             nn.Dropout(0.1))  # output 8\n",
        "\n",
        "#         self.convblock9 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), stride=2, bias=False))  # output 4\n",
        "\n",
        "#         self.GAP = nn.AvgPool2d(kernel_size=1, stride=1)\n",
        "#         self.max_action = max_action\n",
        "#         self.action_dim = action_dim\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.convblock1(x)\n",
        "#         x = self.convblock2(x)\n",
        "#         x = self.convblock3(x)\n",
        "#         x = self.convblock4(x)\n",
        "#         x = self.convblock5(x)\n",
        "#         x = self.convblock6(x)\n",
        "#         x = self.convblock7(x)\n",
        "#         x = self.convblock8(x)\n",
        "#         x = self.convblock9(x)\n",
        "#         x = self.GAP(x)\n",
        "#         x = x.view(-1, 16)\n",
        "#         x = self.max_action * torch.tanh(nn.Linear(x, self.action_dim))\n",
        "#         return x\n",
        "\n",
        "# class Critic(nn.Module):\n",
        "#     def __init__(self, state_dim, action_dim):\n",
        "#         super(Critic, self).__init__()\n",
        "#         # Define first Critic neural network\n",
        "#         # Action_dim angle after GAP as put on same dimension\n",
        "#         self.convblock1 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=state_dim, out_channels=8, kernel_size=(3, 3), padding=(1, 1), bias=False),\n",
        "#             nn.ReLU(),\n",
        "#             nn.BatchNorm2d(8),\n",
        "#             nn.Dropout(0.1))  # output_size = 32\n",
        "\n",
        "#         # CONVOLUTION BLOCK 1\n",
        "#         self.convblock2 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), padding=(1, 1), bias=False),\n",
        "#             nn.ReLU(),\n",
        "#             nn.BatchNorm2d(16),\n",
        "#             nn.Dropout(0.1))  # output_size = 32\n",
        "\n",
        "#         # TRANSITION BLOCK 1\n",
        "#         self.convblock3 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), stride=2, padding=0, bias=False),\n",
        "#             nn.ReLU(),\n",
        "#             nn.BatchNorm2d(16),\n",
        "#             nn.Dropout(0.1))  # output 16\n",
        "\n",
        "#         self.convblock4 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=(1, 1), bias=False),\n",
        "#             nn.ReLU(),\n",
        "#             nn.BatchNorm2d(16),\n",
        "#             nn.Dropout(0.1))  # output 16\n",
        "\n",
        "#         self.convblock5 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=(1, 1), bias=False),\n",
        "#             nn.ReLU(),\n",
        "#             nn.BatchNorm2d(16),\n",
        "#             nn.Dropout(0.1))  # output 16\n",
        "\n",
        "#         self.convblock6 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), stride=2, bias=False),\n",
        "#             nn.ReLU(),\n",
        "#             nn.BatchNorm2d(16),\n",
        "#             nn.Dropout(0.1))  # output 8\n",
        "\n",
        "#         self.convblock7 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=(1, 1), bias=False),\n",
        "#             nn.ReLU(),\n",
        "#             nn.BatchNorm2d(16),\n",
        "#             nn.Dropout(0.1))  # output 8\n",
        "\n",
        "#         self.convblock8 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=(1, 1), bias=False),\n",
        "#             nn.ReLU(),\n",
        "#             nn.BatchNorm2d(16),\n",
        "#             nn.Dropout(0.1))  # output 8\n",
        "\n",
        "#         self.convblock9 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), stride=2, bias=False))  # output 4\n",
        "\n",
        "#         self.GAP1 = nn.AvgPool2d(kernel_size=1, stride=1)\n",
        "#         self.Linear1 = nn.Linear(17, 1)\n",
        "\n",
        "#         # Define the second Critic neural network\n",
        "#         self.convblock11 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=state_dim, out_channels=8, kernel_size=(3, 3), padding=(1, 1), bias=False),\n",
        "#             nn.ReLU(),\n",
        "#             nn.BatchNorm2d(8),\n",
        "#             nn.Dropout(0.1))  # output_size = 32\n",
        "\n",
        "#         # CONVOLUTION BLOCK 1\n",
        "#         self.convblock22 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), padding=(1, 1), bias=False),\n",
        "#             nn.ReLU(),\n",
        "#             nn.BatchNorm2d(16),\n",
        "#             nn.Dropout(0.1))  # output_size = 30\n",
        "\n",
        "#         self.convblock33 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), stride=2, padding=0, bias=False),\n",
        "#             nn.ReLU(),\n",
        "#             nn.BatchNorm2d(16),\n",
        "#             nn.Dropout(0.1))  # output 16\n",
        "\n",
        "#         self.convblock44 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=(1, 1), bias=False),\n",
        "#             nn.ReLU(),\n",
        "#             nn.BatchNorm2d(16),\n",
        "#             nn.Dropout(0.1))  # output 16\n",
        "\n",
        "#         self.convblock55 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=(1, 1), bias=False),\n",
        "#             nn.ReLU(),\n",
        "#             nn.BatchNorm2d(16),\n",
        "#             nn.Dropout(0.1))  # output 16\n",
        "\n",
        "#         self.convblock66 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), stride=2, bias=False),\n",
        "#             nn.ReLU(),\n",
        "#             nn.BatchNorm2d(16),\n",
        "#             nn.Dropout(0.1))  # output 8\n",
        "\n",
        "#         self.convblock77 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=(1, 1), bias=False),\n",
        "#             nn.ReLU(),\n",
        "#             nn.BatchNorm2d(16),\n",
        "#             nn.Dropout(0.1))  # output 8\n",
        "\n",
        "#         self.convblock88 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=(1, 1), bias=False),\n",
        "#             nn.ReLU(),\n",
        "#             nn.BatchNorm2d(16),\n",
        "#             nn.Dropout(0.1))  # output 8\n",
        "\n",
        "#         # OUTPUT BLOCK\n",
        "#         self.convblock99 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), stride=2, bias=False))  # output 4\n",
        "\n",
        "#         self.GAP2 = nn.AvgPool2d(kernel_size=1, stride=1)\n",
        "#         self.Linear2 = nn.Linear(17, 1)\n",
        "\n",
        "#     def forward(self, x, u):\n",
        "#         # Forward-Propogation on the first Critic Neural network\n",
        "#         x1 = self.convblock1(x)\n",
        "#         x1 = self.convblock2(x1)\n",
        "#         x1 = self.convblock3(x1)\n",
        "#         x1 = self.convblock4(x1)\n",
        "#         x1 = self.convblock5(x1)\n",
        "#         x1 = self.convblock6(x1)\n",
        "#         x1 = self.convblock7(x1)\n",
        "#         x1 = self.convblock8(x1)\n",
        "#         x1 = self.convblock9(x1)\n",
        "#         x1 = self.GAP1(x1)\n",
        "#         x1 = x1.view(-1, 16)\n",
        "#         x1 = torch.cat([x1, u], 1)\n",
        "#         x1 = self.Linear1(x1, 1)\n",
        "\n",
        "#         # Forward-Propagation on the second Critic Neural Network\n",
        "#         x2 = self.convblock11(x)\n",
        "#         x2 = self.convblock22(x2)\n",
        "#         x2 = self.convblock33(x2)\n",
        "#         x2 = self.convblock44(x2)\n",
        "#         x2 = self.convblock55(x2)\n",
        "#         x2 = self.convblock66(x2)\n",
        "#         x2 = self.convblock77(x2)\n",
        "#         x2 = self.convblock88(x2)\n",
        "#         x2 = self.convblock99(x2)\n",
        "#         x2 = self.GAP2(x2)\n",
        "#         x2 = x2.view(-1, 16)\n",
        "#         x2 = torch.cat([x2, u], 1)\n",
        "#         x2 = self.Linear2(x2, 1)\n",
        "#         return x1, x2\n",
        "\n",
        "#     def Q1(self, x, u):\n",
        "#         x1 = self.convblock1(x)\n",
        "#         x1 = self.convblock2(x1)\n",
        "#         x1 = self.convblock3(x1)\n",
        "#         x1 = self.convblock4(x1)\n",
        "#         x1 = self.convblock5(x1)\n",
        "#         x1 = self.convblock6(x1)\n",
        "#         x1 = self.convblock7(x1)\n",
        "#         x1 = self.convblock8(x1)\n",
        "#         x1 = self.convblock9(x1)\n",
        "#         x1 = self.GAP1(x1)\n",
        "#         x1 = x1.view(-1, 16)\n",
        "#         x1 = torch.cat([x1, u], 1)\n",
        "#         x1 = self.Linear1(x1, 1)\n",
        "#         return x1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4VLNPzfW9wP",
        "colab_type": "code",
        "outputId": "cf1f023f-712f-45ac-a8d2-07fb63c0836a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "model = Actor((1, 32,32),1,10).to(device)\n",
        "summary(model, input_size=(1, 32, 32))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             288\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]             288\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "            Conv2d-7           [-1, 32, 32, 32]           1,024\n",
            "       BatchNorm2d-8           [-1, 32, 32, 32]              64\n",
            "              ReLU-9           [-1, 32, 32, 32]               0\n",
            "           Conv2d-10           [-1, 32, 16, 16]             288\n",
            "      BatchNorm2d-11           [-1, 32, 16, 16]              64\n",
            "             ReLU-12           [-1, 32, 16, 16]               0\n",
            "           Conv2d-13           [-1, 16, 16, 16]             512\n",
            "      BatchNorm2d-14           [-1, 16, 16, 16]              32\n",
            "             ReLU-15           [-1, 16, 16, 16]               0\n",
            "           Conv2d-16           [-1, 16, 16, 16]             144\n",
            "      BatchNorm2d-17           [-1, 16, 16, 16]              32\n",
            "             ReLU-18           [-1, 16, 16, 16]               0\n",
            "           Conv2d-19           [-1, 16, 16, 16]             256\n",
            "      BatchNorm2d-20           [-1, 16, 16, 16]              32\n",
            "             ReLU-21           [-1, 16, 16, 16]               0\n",
            "           Conv2d-22             [-1, 16, 8, 8]             144\n",
            "      BatchNorm2d-23             [-1, 16, 8, 8]              32\n",
            "             ReLU-24             [-1, 16, 8, 8]               0\n",
            "           Conv2d-25             [-1, 16, 8, 8]             256\n",
            "      BatchNorm2d-26             [-1, 16, 8, 8]              32\n",
            "             ReLU-27             [-1, 16, 8, 8]               0\n",
            "        AvgPool2d-28             [-1, 16, 1, 1]               0\n",
            "           Linear-29                  [-1, 100]           1,700\n",
            "           Linear-30                    [-1, 1]             101\n",
            "================================================================\n",
            "Total params: 5,417\n",
            "Trainable params: 5,417\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 2.77\n",
            "Params size (MB): 0.02\n",
            "Estimated Total Size (MB): 2.79\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HRDDce8FXef7"
      },
      "source": [
        "* We build two neural networks for the two Critic models and two neural networks for the two Critic targets\n",
        "* Uses common functions defined above\n",
        "* State and Actor inputs are concatenated before FC (fully connected) layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5svIwtleSxLW",
        "colab": {}
      },
      "source": [
        "# Creating the architecture of the Neural Network\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        # Defining the first Critic neural network\n",
        "        self.c1_model = state_model()\n",
        "        self.c1_fc1 = nn.Linear(16 + action_dim, 100)\n",
        "        self.c1_fc2 = nn.Linear(100, 1)\n",
        "\n",
        "        # Defining the second Critic neural network\n",
        "        self.c2_model = state_model()\n",
        "        self.c2_fc1 = nn.Linear(16 + action_dim, 100)\n",
        "        self.c2_fc2 = nn.Linear(100, 1)\n",
        "\n",
        "    def forward(self, x, u):\n",
        "      # xu = torch.cat([x, u], 2)\n",
        "      # Forwad-Propagation on the first Critic Neural Network\n",
        "      x1 = self.c1_model(x)\n",
        "      x1 = x1.view(-1, 16)\n",
        "      x1 = torch.cat([x1, u], 1)\n",
        "      x1 = self.c1_fc2(self.c1_fc1(x1))\n",
        "\n",
        "      # Forward-Propagation on the second Critic Neural Network\n",
        "      x2 = self.c2_model(x)\n",
        "      x2 = x2.view(-1, 16)\n",
        "      x2 = torch.cat([x2, u], 1)\n",
        "      x2 = self.c2_fc2(self.c2_fc1(x2))\n",
        "      return x1, x2\n",
        "\n",
        "    def Q1(self, x, u):\n",
        "      x1 = self.c1_model(x)\n",
        "      x1 = x1.view(-1, 16)\n",
        "      x1 = torch.cat([x1, u], 1)\n",
        "      x1 = self.c1_fc2(self.c1_fc1(x1))\n",
        "      return x1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQkGTKaYFt5n",
        "colab_type": "text"
      },
      "source": [
        "* Below torch summary fails to calculate parameters\n",
        "* This is a known torchsummary issue"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPFqU04lYoUM",
        "colab_type": "code",
        "outputId": "4b25c7f1-7b2e-49d6-c8cd-23d84baac8fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Critic(2,1).to(device)\n",
        "summary(model, input_size=[(1, 32, 32), (1,)])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             288\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]             288\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "            Conv2d-7           [-1, 32, 32, 32]           1,024\n",
            "       BatchNorm2d-8           [-1, 32, 32, 32]              64\n",
            "              ReLU-9           [-1, 32, 32, 32]               0\n",
            "           Conv2d-10           [-1, 32, 16, 16]             288\n",
            "      BatchNorm2d-11           [-1, 32, 16, 16]              64\n",
            "             ReLU-12           [-1, 32, 16, 16]               0\n",
            "           Conv2d-13           [-1, 16, 16, 16]             512\n",
            "      BatchNorm2d-14           [-1, 16, 16, 16]              32\n",
            "             ReLU-15           [-1, 16, 16, 16]               0\n",
            "           Conv2d-16           [-1, 16, 16, 16]             144\n",
            "      BatchNorm2d-17           [-1, 16, 16, 16]              32\n",
            "             ReLU-18           [-1, 16, 16, 16]               0\n",
            "           Conv2d-19           [-1, 16, 16, 16]             256\n",
            "      BatchNorm2d-20           [-1, 16, 16, 16]              32\n",
            "             ReLU-21           [-1, 16, 16, 16]               0\n",
            "           Conv2d-22             [-1, 16, 8, 8]             144\n",
            "      BatchNorm2d-23             [-1, 16, 8, 8]              32\n",
            "             ReLU-24             [-1, 16, 8, 8]               0\n",
            "           Conv2d-25             [-1, 16, 8, 8]             256\n",
            "      BatchNorm2d-26             [-1, 16, 8, 8]              32\n",
            "             ReLU-27             [-1, 16, 8, 8]               0\n",
            "        AvgPool2d-28             [-1, 16, 1, 1]               0\n",
            "           Linear-29                  [-1, 100]           1,800\n",
            "           Linear-30                    [-1, 1]             101\n",
            "           Conv2d-31           [-1, 32, 32, 32]             288\n",
            "      BatchNorm2d-32           [-1, 32, 32, 32]              64\n",
            "             ReLU-33           [-1, 32, 32, 32]               0\n",
            "           Conv2d-34           [-1, 32, 32, 32]             288\n",
            "      BatchNorm2d-35           [-1, 32, 32, 32]              64\n",
            "             ReLU-36           [-1, 32, 32, 32]               0\n",
            "           Conv2d-37           [-1, 32, 32, 32]           1,024\n",
            "      BatchNorm2d-38           [-1, 32, 32, 32]              64\n",
            "             ReLU-39           [-1, 32, 32, 32]               0\n",
            "           Conv2d-40           [-1, 32, 16, 16]             288\n",
            "      BatchNorm2d-41           [-1, 32, 16, 16]              64\n",
            "             ReLU-42           [-1, 32, 16, 16]               0\n",
            "           Conv2d-43           [-1, 16, 16, 16]             512\n",
            "      BatchNorm2d-44           [-1, 16, 16, 16]              32\n",
            "             ReLU-45           [-1, 16, 16, 16]               0\n",
            "           Conv2d-46           [-1, 16, 16, 16]             144\n",
            "      BatchNorm2d-47           [-1, 16, 16, 16]              32\n",
            "             ReLU-48           [-1, 16, 16, 16]               0\n",
            "           Conv2d-49           [-1, 16, 16, 16]             256\n",
            "      BatchNorm2d-50           [-1, 16, 16, 16]              32\n",
            "             ReLU-51           [-1, 16, 16, 16]               0\n",
            "           Conv2d-52             [-1, 16, 8, 8]             144\n",
            "      BatchNorm2d-53             [-1, 16, 8, 8]              32\n",
            "             ReLU-54             [-1, 16, 8, 8]               0\n",
            "           Conv2d-55             [-1, 16, 8, 8]             256\n",
            "      BatchNorm2d-56             [-1, 16, 8, 8]              32\n",
            "             ReLU-57             [-1, 16, 8, 8]               0\n",
            "        AvgPool2d-58             [-1, 16, 1, 1]               0\n",
            "           Linear-59                  [-1, 100]           1,800\n",
            "           Linear-60                    [-1, 1]             101\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-cc92e98592ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;31m# assume 4 bytes/number (float on cuda).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mtotal_input_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0mtotal_output_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtotal_output\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# x2 for gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mtotal_params_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mprod\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2960\u001b[0m     \"\"\"\n\u001b[1;32m   2961\u001b[0m     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\n\u001b[0;32m-> 2962\u001b[0;31m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0m\u001b[1;32m   2963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'tuple'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NzIDuONodenW"
      },
      "source": [
        "## Define T3D for Training Process whole process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zzd0H1xukdKe",
        "colab": {}
      },
      "source": [
        "# Building the whole Training Process into a class\n",
        "class T3D(object):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.max_action = torch.Tensor(max_action).to(device)\n",
        "    self.actor = Actor(state_dim, action_dim, self.max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, self.max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    \n",
        "  def select_action(self, state):\n",
        "    state = state.unsqueeze(0).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iterations):\n",
        "      # We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)#.squeeze(1)\n",
        "\n",
        "      # We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action[0], self.max_action[0])\n",
        "\n",
        "      # The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "      \n",
        "      # We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      # The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      \n",
        "      # We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      # We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      # Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gGuKmH_ijf7U"
      },
      "source": [
        "# Set the Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HFj6wbAo97lk",
        "colab": {}
      },
      "source": [
        "env_name = \"SelfDrivingCarEnv\" # Name of a environment (set it to any Continous environment you want)\n",
        "seed = 0 # Random seed number\n",
        "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
        "eval_freq = 2e4 # How often the evaluation step is performed (after how many timesteps)\n",
        "max_timesteps = 5e5 # Total number of iterations/timesteps\n",
        "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
        "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
        "batch_size = 100 # Size of the batch\n",
        "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
        "tau = 0.005 # Target network update rate\n",
        "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
        "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
        "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Hjwf2HCol3XP"
      },
      "source": [
        "## We create a file name for the two saved models: the Actor and Critic models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1fyH8N5z-o3o",
        "outputId": "5a498f37-f810-42e1-9075-6148ba7e0d80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "file_name = \"%s_%s_%s\" % (\"T3D\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: T3D_SelfDrivingCarEnv_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kop-C96Aml8O"
      },
      "source": [
        "## We create a folder inside which will be saved the trained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Src07lvY-zXb",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(\"./results\"):\n",
        "  os.makedirs(\"./results\")\n",
        "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
        "  os.makedirs(\"./pytorch_models\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjXU4F5qV69_",
        "colab_type": "text"
      },
      "source": [
        "# Download Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFxh7LT1V7Oe",
        "colab_type": "code",
        "outputId": "d4382474-2d77-414c-c2eb-596b23749901",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "source": [
        "!wget -O MASK1.png https://drive.google.com/uc\\?export\\=view\\&id\\=1eHsgsEdvdUMZjSR5XUhA2kFgdPK3qSN3"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-07 13:17:02--  https://drive.google.com/uc?export=view&id=1eHsgsEdvdUMZjSR5XUhA2kFgdPK3qSN3\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.5.206, 2607:f8b0:4007:803::200e\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.5.206|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0c-60-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ol6e3psahjjs33d6jts81e432cu1lc1s/1588857375000/09305796575597883680/*/1eHsgsEdvdUMZjSR5XUhA2kFgdPK3qSN3?e=view [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-05-07 13:17:02--  https://doc-0c-60-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ol6e3psahjjs33d6jts81e432cu1lc1s/1588857375000/09305796575597883680/*/1eHsgsEdvdUMZjSR5XUhA2kFgdPK3qSN3?e=view\n",
            "Resolving doc-0c-60-docs.googleusercontent.com (doc-0c-60-docs.googleusercontent.com)... 216.58.217.193, 2607:f8b0:4007:808::2001\n",
            "Connecting to doc-0c-60-docs.googleusercontent.com (doc-0c-60-docs.googleusercontent.com)|216.58.217.193|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 103090 (101K) [image/png]\n",
            "Saving to: ‘MASK1.png’\n",
            "\n",
            "MASK1.png           100%[===================>] 100.67K  --.-KB/s    in 0.09s   \n",
            "\n",
            "2020-05-07 13:17:03 (1.14 MB/s) - ‘MASK1.png’ saved [103090/103090]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR0P41ogGZcN",
        "colab_type": "text"
      },
      "source": [
        "# Define helper functions for Cropping Image\n",
        "* Below are helper functions to crop the image and perform rotation on it\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxN_v2aAppds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "from PIL import Image, ImageOps\n",
        "import matplotlib.markers as mmarkers\n",
        "\n",
        "def rotate_img(img, angle):\n",
        "  im1 = img.convert('RGBA')\n",
        "  rot = im1.rotate(angle)\n",
        "  # a white image same size as rotated image\n",
        "  fff = Image.new('RGBA', rot.size, (255,)*4)\n",
        "  # create a composite image using the alpha layer of rot as a mask\n",
        "  return Image.composite(rot, fff, rot).convert('L')\n",
        "\n",
        "def show_img(ax, img, title=\"\"):\n",
        "  np_img = np.asarray(img)/255\n",
        "  np_img = np_img.astype(int)\n",
        "  ax.imshow(np_img, cmap='gray', vmin=0, vmax=1)\n",
        "  if title != \"\":\n",
        "    ax.set_title(title)\n",
        "\n",
        "def crop(img, x, y , angle = 0, crop_size = 100, scale_size = 32):\n",
        "    img = np.asarray(img)\n",
        "    def pad_with(vector, pad_width, iaxis, kwargs):\n",
        "            pad_value = kwargs.get('padder', 10)\n",
        "            vector[:pad_width[0]] = pad_value\n",
        "            vector[-pad_width[1]:] = pad_value    \n",
        "    img = np.pad(img, crop_size // 2, pad_with, padder=255.)\n",
        "    img_x, img_y = img.shape\n",
        "    \n",
        "    x += crop_size // 2\n",
        "    y += crop_size // 2\n",
        "    center_x = x + 10\n",
        "    center_y = y + 5    \n",
        "\n",
        "    cropped_image = img[center_x - crop_size//2:center_x + crop_size//2,center_y - crop_size//2:center_y + crop_size//2]\n",
        "\n",
        "    res = cv2.resize(cropped_image, dsize=(scale_size,scale_size), interpolation=cv2.INTER_CUBIC)\n",
        "    res = np.expand_dims(res, axis=0)\n",
        "    res = torch.from_numpy(res)\n",
        "    return res\n",
        "\n",
        "def center_crop_img(img, x, y, crop_size):\n",
        "  max_x, max_y = img.size\n",
        "  pad_left, pad_right, pad_bottom, pad_top = 0, 0, 0, 0\n",
        "  start_x = x - int(crop_size/2)\n",
        "  start_y = y - int(crop_size/2)\n",
        "  end_x = x + int(crop_size/2)\n",
        "  end_y = y + int(crop_size/2)\n",
        "  if start_x < 0:\n",
        "      pad_left = -start_x\n",
        "      start_x = 0\n",
        "  if end_x >= max_x:\n",
        "      pad_right = end_x - max_x\n",
        "  if start_y < 0:\n",
        "      pad_top = -start_y\n",
        "      start_y = 0\n",
        "  if end_y >= max_y:\n",
        "      pad_bottom = end_y - max_y\n",
        "  padding = (int(pad_left), int(pad_top), int(pad_right), int(pad_bottom))\n",
        "  new_img = ImageOps.expand(img, padding, fill=255)\n",
        "  crop_img = new_img.crop((start_x, start_y, start_x+crop_size, start_y+crop_size))\n",
        "\n",
        "  return crop_img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qEAzOd47mv1Z"
      },
      "source": [
        "## Environment creation for Simulation \n",
        "* We simulate Kivy environment here. As Kivy doesn't do much apart from Graphics\n",
        "* We maintain x,y position and car's angle. This is rotated based on action\n",
        "* Action here is one-dimensional, which is the amount of degrees the car should rotate\n",
        "* If x,y position is on sand, we set a small velocity, else a slightly high velocity\n",
        "* Our state here corresponds to the cropped portion of current postion as center. This image is rotated to be in the direction of car. This was our network understands car's orientation\n",
        "* Cropping here is done differently. If we directly crop and rotate the image, we may loose information from the edges. Hence we do the following:\n",
        "  * Crop a larger portion of image\n",
        "  * Rotate it to make the cropped image in the direction of car's orientation\n",
        "  * Then crop it to required size\n",
        "* `reset` function picks a random coordinates on road and a random angle and returns state"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFCJs2XJKzIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.markers as mmarkers\n",
        "\n",
        "from PIL import Image as PILImage\n",
        "from kivy.vector import Vector\n",
        "from collections import Counter\n",
        "\n",
        "class CarEnv(object):\n",
        "    def __init__(self):\n",
        "        self.filename = \"MASK1.png\"\n",
        "        img = PILImage.open(self.filename).convert('L')\n",
        "        self.sand = np.asarray(img)/255\n",
        "        self.sand = self.sand.astype(int)\n",
        "        self.max_y, self.max_x = self.sand.shape\n",
        "        self.pos = Vector(int(self.max_x/2), int(self.max_y/2))\n",
        "        self.angle = Vector(0,10).angle(self.pos)\n",
        "        self.velocity = Vector(6, 0)\n",
        "        self.wall_padding = 20\n",
        "        self.rel_pos = Vector(0,0)\n",
        "        self.max_angle = 10\n",
        "        self.max_action = [self.max_angle]\n",
        "        self.crop_size = 100\n",
        "        self.goal_iter = 0\n",
        "        self.goals = [Vector(1650, 130), Vector(160, 390)]\n",
        "        self.last_distance = 0\n",
        "        self.state_dim = (32, 32)\n",
        "        self.action_dim = (1,)\n",
        "        self._max_episode_steps = 4000\n",
        "        # track rewards distribution\n",
        "        self.rewards_distribution = Counter()\n",
        "      \n",
        "    def seed(self, seed):\n",
        "        pass\n",
        "    \n",
        "    def reset(self):\n",
        "        self.angle = np.random.randint(low=0, high=360)\n",
        "        onsand = True\n",
        "        while onsand:\n",
        "          self.pos.x = np.random.randint(low=self.wall_padding, high=self.max_x-self.wall_padding)\n",
        "          self.pos.y = np.random.randint(low=self.wall_padding, high=self.max_y-self.wall_padding)\n",
        "          if self.sand[int(self.pos.y),int(self.pos.x)] <= 0:\n",
        "              onsand = False\n",
        "        self.velocity = Vector(2, 0).rotate(self.angle)\n",
        "        return self.get_state()\n",
        "    \n",
        "    def random_action(self):\n",
        "        rotation = np.random.randint(low=-self.max_angle, high=self.max_angle)\n",
        "        return (rotation,)\n",
        "    \n",
        "    def step(self, action):\n",
        "        rotation = action[0]\n",
        "        self.angle += rotation\n",
        "        self.pos = Vector(*self.velocity) + self.pos\n",
        "        self.pos.x = int(self.pos.x)\n",
        "        self.pos.y = int(self.pos.y)\n",
        "        reward = 0\n",
        "        done = False\n",
        "        current_goal = self.goals[self.goal_iter]\n",
        "\n",
        "        distance = self.pos.distance(current_goal)\n",
        "        if self.sand[int(self.pos.y),int(self.pos.x)] > 0:\n",
        "            self.velocity = Vector(0.5, 0).rotate(self.angle)\n",
        "            reward = -1\n",
        "            tag = \"sand (-1)\"\n",
        "        else: # otherwise\n",
        "            self.velocity = Vector(2, 0).rotate(self.angle)\n",
        "            reward = -0.1\n",
        "            tag = \"road (-0.1)\"\n",
        "            \n",
        "            if distance < self.last_distance:\n",
        "              reward = 1\n",
        "              tag = \"road (+1)\"\n",
        "\n",
        "        if self.pos.x < self.wall_padding:\n",
        "            self.pos.x = self.wall_padding\n",
        "            reward = -5\n",
        "            tag = \"wall (-5)\"\n",
        "            done = True\n",
        "        if self.pos.x > self.max_x - self.wall_padding:\n",
        "            self.pos.x = self.max_x - self.wall_padding\n",
        "            reward = -5\n",
        "            tag = \"wall (-5)\"\n",
        "            done = True\n",
        "        if self.pos.y < self.wall_padding:\n",
        "            self.pos.y = self.wall_padding\n",
        "            reward = -5\n",
        "            tag = \"wall (-5)\"\n",
        "            done = True\n",
        "        if self.pos.y > self.max_y - self.wall_padding:\n",
        "            self.pos.y = self.max_y - self.wall_padding\n",
        "            reward = -5\n",
        "            tag = \"wall (-5)\"\n",
        "            done = True\n",
        "\n",
        "        if distance < 25:\n",
        "            self.goal_iter = (self.goal_iter + 1) % len(self.goals)\n",
        "            goal = self.goals[self.goal_iter]\n",
        "            reward = 10\n",
        "            tag = \"goal (+10)\"\n",
        "            done = True\n",
        "\n",
        "        self.last_distance = distance\n",
        "        self.rewards_distribution[tag] += 1\n",
        "        return self.get_state(), reward, done\n",
        "    \n",
        "    def render(self):\n",
        "        # Create figure and axes\n",
        "        fig, ax = plt.subplots(1, 5, figsize=(30, 6))\n",
        "\n",
        "        # Display the image\n",
        "        ax[0].imshow(self.sand, cmap='gray', vmin=0, vmax=1)\n",
        "        # Create a Rectangle patch\n",
        "        rect = patches.Rectangle(\n",
        "            (self.pos.x - int(self.crop_size/2), self.pos.y - int(self.crop_size/2)),\n",
        "            self.crop_size, self.crop_size,\n",
        "            linewidth=1, edgecolor='r', facecolor='none'\n",
        "        )\n",
        "        # Add the patch to the Axes\n",
        "        ax[0].add_patch(rect)\n",
        "        ax[0].set_title(\"x=%d,y=%d,angle=%d\" % (self.pos.x, self.pos.y, self.angle))\n",
        "        \n",
        "        marker = mmarkers.MarkerStyle(marker=\"$ \\\\rightarrow$\")\n",
        "        marker._transform = marker.get_transform().rotate_deg(self.angle)\n",
        "        ax[0].scatter(self.pos.x, self.pos.y, s=50, c='red', marker=marker)\n",
        "        self.get_state(ax).cpu().numpy()\n",
        "        plt.show()\n",
        "        \n",
        "    def get_state(self, ax=None):\n",
        "        resize = T.Compose([T.ToPILImage(),\n",
        "                            T.Resize(self.state_dim[0], interpolation=Image.CUBIC),\n",
        "                            T.ToTensor()])\n",
        "        \n",
        "        img = Image.open(self.filename).convert('L')\n",
        "\n",
        "        # If we directly crop and rotate the image, we may loose information\n",
        "        # from the edges. Hence we do the following:\n",
        "        #   * Crop a larger portion of image\n",
        "        #   * Rotate it to make the cropped image in the direction\n",
        "        #     of car's orientation\n",
        "        #   * Then crop it to required size\n",
        "        crop_img = center_crop_img(img, self.pos.x, self.pos.y, self.crop_size*3)\n",
        "        if ax is not None:\n",
        "          show_img(ax[1], crop_img, \"large crop\")\n",
        "\n",
        "        r_img = rotate_img(crop_img, -self.angle)\n",
        "        if ax is not None:\n",
        "          show_img(ax[2], r_img, \"rotated crop\")\n",
        "\n",
        "        r_img_x, r_img_y = r_img.size\n",
        "        crop_img = center_crop_img(r_img, int(r_img_x/2), int(r_img_y/2), self.crop_size)\n",
        "        if ax is not None:\n",
        "          show_img(ax[3], crop_img, \"final crop\")\n",
        "\n",
        "        np_img = np.asarray(crop_img)/255\n",
        "        np_img = np_img.astype(int)\n",
        "        screen = np.ascontiguousarray(np_img, dtype=np.float32) \n",
        "        screen = torch.from_numpy(screen)\n",
        "        screen = resize(screen)\n",
        "        if ax is not None:\n",
        "            np_img = screen.squeeze(0).numpy()\n",
        "            np_img = np_img.astype(int)\n",
        "            ax[4].imshow(np_img, cmap='gray', vmin=0, vmax=1)\n",
        "            marker = mmarkers.MarkerStyle(marker=\"$ \\\\rightarrow$\")\n",
        "            ax[4].scatter(self.state_dim[0]/2, self.state_dim[1]/2, s=100, c='red', marker=marker)\n",
        "            ax[4].set_title(\"final resized img\")\n",
        "        return screen.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIZ7-z_ARu7Y",
        "colab_type": "text"
      },
      "source": [
        "# Visualizing Environment\n",
        "* Here we crop large portion of image from postion x,y. Then rotate it and then crop it again to required size. Then resize it to required final size\n",
        "* It is visualized below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "VsSMlufaRYw3",
        "colab_type": "code",
        "outputId": "fd80137d-d222-4aeb-d25b-9bf85e22b2f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        }
      },
      "source": [
        "env = CarEnv()\n",
        "env.reset()\n",
        "env.render()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 2160x432 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvmcYjdZR-FW",
        "colab_type": "text"
      },
      "source": [
        "# Small animation to understand env step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbXaXY1rRYw5",
        "colab_type": "code",
        "outputId": "f9ce6e05-1ff7-4a4c-d878-a674fe7fc796",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "\n",
        "plt.figure()\n",
        "env = CarEnv()\n",
        "env.reset()\n",
        "for i in range(20):\n",
        "    img = env.get_state().cpu().squeeze(0).numpy()\n",
        "    plt.imshow(img, cmap='gray', animated=True, vmin=0, vmax=1)\n",
        "    plt.show()\n",
        "    sleep(0.2)\n",
        "    clear_output(wait=True)\n",
        "    vel = np.random.randint(low=0.5, high=5)\n",
        "    angle = np.random.randint(low=-10, high=+10)\n",
        "    env.step((angle,))\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5YdPG4HXnNsh"
      },
      "source": [
        "## Set seeds and get the necessary information on the states and actions in the chosen environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z3RufYec_ADj",
        "colab": {}
      },
      "source": [
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.state_dim\n",
        "action_dim = env.action_dim[0]\n",
        "max_action = env.max_action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HWEgDAQxnbem"
      },
      "source": [
        "## Create the policy network\n",
        "* This sets up all the networks: Actor, Actor-Target, Critic1, Critic1-Target, Critic2, Critic2-Target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wTVvG7F8_EWg",
        "colab": {}
      },
      "source": [
        "policy = T3D(state_dim, action_dim, max_action)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZI60VN2Unklh"
      },
      "source": [
        "## Initialize the Experience ReplayBuffer memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sd-ZsdXR_LgV",
        "colab": {}
      },
      "source": [
        "replay_buffer = ReplayBuffer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1L4Jra3KwED",
        "colab_type": "text"
      },
      "source": [
        "## Verify policy network before training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwS4ffh4VKUN",
        "colab_type": "code",
        "outputId": "af425a8f-af41-4f5e-866a-f1cfab2e8d06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "obs = env.reset()\n",
        "action = policy.select_action(obs)\n",
        "new_obs, reward, done,  = env.step(action)\n",
        "print(obs.shape, new_obs.shape, action, reward, done)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 32, 32]) torch.Size([1, 32, 32]) [1.2357162] -0.1 False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ka-ZRtQvjBex"
      },
      "source": [
        "## We make a function that evaluates the policy by calculating its average reward over 10 episodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qabqiYdp9wDM",
        "colab": {}
      },
      "source": [
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    episode_timesteps = 0\n",
        "    while not done:\n",
        "      action = policy.select_action(obs)\n",
        "      obs, reward, done = env.step(action)\n",
        "      episode_timesteps += 1\n",
        "      avg_reward += reward\n",
        "      if episode_timesteps + 1 == env._max_episode_steps:\n",
        "        done = True\n",
        "\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QYOpCyiDnw7s"
      },
      "source": [
        "## Define a list where all the evaluation results over 10 episodes are stored"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dhC_5XJ__Orp",
        "colab": {}
      },
      "source": [
        "evaluations = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "31n5eb03p-Fm"
      },
      "source": [
        "## Initialize the variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1vN5EvxK_QhT",
        "colab": {}
      },
      "source": [
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "done = True\n",
        "t0 = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q9gsjvtPqLgT"
      },
      "source": [
        "## Training process\n",
        "* As part of training, we show following output:\n",
        "  * Total number of timesteps taken, Episode number, Length of current episod, Total reward accumulated during this episode and Time taken to finish this episode\n",
        "```\n",
        "Total Timesteps: 561 Episode Num: 1 Episode Len: 561 Reward: -449.7 Time Taken: 8.64677596092 secs\n",
        "```\n",
        "  * Distribution of rewards. This is not from replay memory, but from the actual steps taken during this run of episode. It shows how much and from where rewards were allocated. Like:\n",
        "    * `road (+1)` means it got +1 reward for being on road and for going towards final goal\n",
        "    * `road (-0.1)` means it got -0.1 reward for being on road but not going towards final goal (this is living penalty)\n",
        "    * `sand (-1)` means it got -1 reward for being on sand\n",
        "    * `wall (-5)` means it got -5 reward for hitting the wall\n",
        "    * Note: we assign a reward of -10 if we hit max episode steps\n",
        "    ```\n",
        "('Rewards Distribution: ', [('road (+1)', 33), ('road (-0.1)', 76), ('sand (-1)', 472), ('wall (-5)', 1)])\n",
        "    ```\n",
        "  * Amound of time taken by `policy.train` to finish \n",
        "```\n",
        "Training took 24.9262328148 secs\n",
        "```\n",
        "  * Path. Set of coordinates noted at every 100 steps, to verify that the network starts from a random point and covers lot of new points, rather than being stuck in a loop \n",
        "```\n",
        "('Path', [[783, 379], [784, 377], [757, 255], [647, 167], [598, 167], [545, 192], [561, 86]])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPn31gvhKSAp",
        "colab_type": "code",
        "outputId": "0ced8967-9189-4038-887f-1974780e0a56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import time\n",
        "\n",
        "max_timesteps = 500000\n",
        "start_time = time.time()\n",
        "path = []\n",
        "# We start the main loop over 500,000 timesteps\n",
        "while total_timesteps < max_timesteps:\n",
        "  \n",
        "  # If the episode is done\n",
        "  if done:\n",
        "\n",
        "    # If we are not at the very beginning, we start the training process of the model\n",
        "    if total_timesteps != 0:\n",
        "      print(\"Total Timesteps: {} Episode Num: {} Episode Len: {} Reward: {} Time Taken: {} secs\".format(\n",
        "              total_timesteps, episode_num, episode_timesteps,\n",
        "              episode_reward, time.time() - start_time))\n",
        "      print(\"Rewards Distribution: \", sorted(env.rewards_distribution.items()))\n",
        "      env.rewards_distribution = Counter()\n",
        "      start_time = time.time()\n",
        "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "      print(\"Training took {} secs\".format(time.time() - start_time))\n",
        "      print(\"Path\", path)\n",
        "      print(\"\")\n",
        "      start_time = time.time()\n",
        "\n",
        "    # We evaluate the episode and we save the policy\n",
        "    if timesteps_since_eval >= eval_freq:\n",
        "      timesteps_since_eval %= eval_freq\n",
        "      evaluations.append(evaluate_policy(policy))\n",
        "      policy.save(file_name, directory=\"./pytorch_models\")\n",
        "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
        "      start_time = time.time()\n",
        "    \n",
        "    # When the training step is done, we reset the state of the environment\n",
        "    obs = env.reset()\n",
        "    path = [env.pos]\n",
        "    \n",
        "    # Set the Done to False\n",
        "    done = False\n",
        "    \n",
        "    # Set rewards and episode timesteps to zero\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num += 1\n",
        "  \n",
        "  # Before 10000 timesteps, we play random actions\n",
        "  if total_timesteps < start_timesteps:\n",
        "    action = env.random_action()\n",
        "  else: # After 10000 timesteps, we switch to the model\n",
        "    action = policy.select_action(obs)\n",
        "    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
        "    if expl_noise != 0:\n",
        "      action = (action + np.random.normal(0, expl_noise, size=env.action_dim)).clip(-env.max_angle, env.max_angle)\n",
        "  \n",
        "  # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
        "  new_obs, reward, done = env.step(action)\n",
        "  if episode_timesteps % 100 == 0:\n",
        "    path.append(env.pos)\n",
        "  #if fill_replay:\n",
        "  #  print(env.pos, action, reward, done)\n",
        "  \n",
        "  # We check if the episode is done\n",
        "  # done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
        "  if episode_timesteps + 1 == env._max_episode_steps:\n",
        "    done = True\n",
        "    reward = -10\n",
        "    env.rewards_distribution[\"max-episodes (-10)\"] += 1\n",
        "  done_bool = float(done)\n",
        "  \n",
        "  # We increase the total reward\n",
        "  episode_reward += reward\n",
        "  # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
        "  replay_buffer.add((obs.cpu(), new_obs.cpu(), action, reward, done_bool))\n",
        "\n",
        "  # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
        "  obs = new_obs\n",
        "  episode_timesteps += 1\n",
        "  total_timesteps += 1\n",
        "  timesteps_since_eval += 1\n",
        "\n",
        "# We add the last policy evaluation to our list of evaluations and we save our model\n",
        "evaluations.append(evaluate_policy(policy))\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Timesteps: 561 Episode Num: 1 Episode Len: 561 Reward: -448.6000000000006 Time Taken: 8.853923082351685 secs\n",
            "Rewards Distribution:  [('road (+1)', 33), ('road (-0.1)', 58), ('sand (-1)', 490), ('wall (-5)', 1)]\n",
            "Training took 23.693923234939575 secs\n",
            "Path [[783, 379], [784, 377], [757, 255], [647, 167], [598, 167], [545, 192], [561, 86]]\n",
            "\n",
            "Total Timesteps: 1774 Episode Num: 2 Episode Len: 1213 Reward: -901.1000000000003 Time Taken: 19.245119094848633 secs\n",
            "Rewards Distribution:  [('road (+1)', 108), ('road (-0.1)', 111), ('sand (-1)', 993), ('wall (-5)', 1)]\n",
            "Training took 51.04214096069336 secs\n",
            "Path [[836, 598], [837, 598], [913, 573], [892, 473], [755, 379], [628, 295], [517, 293], [417, 277], [336, 294], [236, 246], [136, 232], [65, 242], [87, 152], [31, 41]]\n",
            "\n",
            "Total Timesteps: 2399 Episode Num: 3 Episode Len: 625 Reward: -476.09999999999997 Time Taken: 9.922103881835938 secs\n",
            "Rewards Distribution:  [('road (+1)', 22), ('road (-0.1)', 121), ('sand (-1)', 481), ('wall (-5)', 1)]\n",
            "Training took 26.30414628982544 secs\n",
            "Path [[692, 298], [690, 297], [585, 183], [486, 85], [351, 91], [239, 119], [161, 132], [156, 43]]\n",
            "\n",
            "Total Timesteps: 2716 Episode Num: 4 Episode Len: 317 Reward: -243.39999999999984 Time Taken: 5.005812168121338 secs\n",
            "Rewards Distribution:  [('road (+1)', 19), ('road (-0.1)', 44), ('sand (-1)', 253), ('wall (-5)', 1)]\n",
            "Training took 13.299798488616943 secs\n",
            "Path [[202, 366], [202, 367], [220, 312], [157, 208], [50, 150]]\n",
            "\n",
            "Total Timesteps: 2851 Episode Num: 5 Episode Len: 135 Reward: -124.39999999999992 Time Taken: 2.1516730785369873 secs\n",
            "Rewards Distribution:  [('road (+1)', 1), ('road (-0.1)', 14), ('sand (-1)', 119), ('wall (-5)', 1)]\n",
            "Training took 5.668477535247803 secs\n",
            "Path [[340, 170], [340, 168], [323, 56]]\n",
            "\n",
            "Total Timesteps: 3766 Episode Num: 6 Episode Len: 915 Reward: -767.3000000000023 Time Taken: 14.453722476959229 secs\n",
            "Rewards Distribution:  [('road (+1)', 7), ('road (-0.1)', 153), ('sand (-1)', 754), ('wall (-5)', 1)]\n",
            "Training took 38.46423578262329 secs\n",
            "Path [[946, 566], [947, 564], [954, 457], [849, 385], [730, 398], [637, 429], [527, 386], [384, 328], [273, 253], [171, 143], [71, 33]]\n",
            "\n",
            "Total Timesteps: 3810 Episode Num: 7 Episode Len: 44 Reward: -14 Time Taken: 0.7204339504241943 secs\n",
            "Rewards Distribution:  [('road (+1)', 17), ('sand (-1)', 26), ('wall (-5)', 1)]\n",
            "Training took 1.8513085842132568 secs\n",
            "Path [[804, 55], [806, 55]]\n",
            "\n",
            "Total Timesteps: 4577 Episode Num: 8 Episode Len: 767 Reward: -663.8000000000003 Time Taken: 12.152027368545532 secs\n",
            "Rewards Distribution:  [('road (+1)', 41), ('road (-0.1)', 28), ('sand (-1)', 697), ('wall (-5)', 1)]\n",
            "Training took 32.440993309020996 secs\n",
            "Path [[654, 423], [652, 422], [547, 417], [472, 417], [471, 407], [478, 295], [381, 207], [319, 184], [344, 68]]\n",
            "\n",
            "Total Timesteps: 4844 Episode Num: 9 Episode Len: 267 Reward: -230.49999999999997 Time Taken: 4.317676782608032 secs\n",
            "Rewards Distribution:  [('road (+1)', 18), ('road (-0.1)', 5), ('sand (-1)', 243), ('wall (-5)', 1)]\n",
            "Training took 11.306434392929077 secs\n",
            "Path [[1086, 30], [1085, 31], [1060, 38], [1051, 39]]\n",
            "\n",
            "Total Timesteps: 5283 Episode Num: 10 Episode Len: 439 Reward: -216.29999999999984 Time Taken: 7.014513254165649 secs\n",
            "Rewards Distribution:  [('road (+1)', 67), ('road (-0.1)', 103), ('sand (-1)', 268), ('wall (-5)', 1)]\n",
            "Training took 18.5360689163208 secs\n",
            "Path [[873, 190], [871, 190], [718, 122], [604, 89], [591, 163], [604, 62]]\n",
            "\n",
            "Total Timesteps: 5888 Episode Num: 11 Episode Len: 605 Reward: -540.4000000000003 Time Taken: 9.627033710479736 secs\n",
            "Rewards Distribution:  [('road (+1)', 28), ('road (-0.1)', 14), ('sand (-1)', 562), ('wall (-5)', 1)]\n",
            "Training took 25.59664034843445 secs\n",
            "Path [[349, 280], [347, 280], [245, 280], [184, 280], [184, 251], [194, 204], [212, 122], [122, 20]]\n",
            "\n",
            "Total Timesteps: 6178 Episode Num: 12 Episode Len: 290 Reward: -156.0999999999997 Time Taken: 4.616147518157959 secs\n",
            "Rewards Distribution:  [('road (+1)', 46), ('road (-0.1)', 51), ('sand (-1)', 192), ('wall (-5)', 1)]\n",
            "Training took 12.25499963760376 secs\n",
            "Path [[556, 308], [557, 308], [593, 269], [565, 144]]\n",
            "\n",
            "Total Timesteps: 6584 Episode Num: 13 Episode Len: 406 Reward: -387.3000000000004 Time Taken: 6.446633577346802 secs\n",
            "Rewards Distribution:  [('road (+1)', 1), ('road (-0.1)', 23), ('sand (-1)', 381), ('wall (-5)', 1)]\n",
            "Training took 17.168233156204224 secs\n",
            "Path [[1068, 174], [1066, 174], [1015, 174], [1015, 174], [1015, 142], [995, 28]]\n",
            "\n",
            "Total Timesteps: 7103 Episode Num: 14 Episode Len: 519 Reward: -468.80000000000007 Time Taken: 8.240119695663452 secs\n",
            "Rewards Distribution:  [('road (+1)', 19), ('road (-0.1)', 18), ('sand (-1)', 481), ('wall (-5)', 1)]\n",
            "Training took 21.92655658721924 secs\n",
            "Path [[114, 278], [115, 279], [105, 294], [105, 294], [105, 259], [88, 148], [88, 37]]\n",
            "\n",
            "Total Timesteps: 7867 Episode Num: 15 Episode Len: 764 Reward: -683.0999999999999 Time Taken: 12.154541015625 secs\n",
            "Rewards Distribution:  [('road (+1)', 33), ('road (-0.1)', 21), ('sand (-1)', 709), ('wall (-5)', 1)]\n",
            "Training took 32.28008246421814 secs\n",
            "Path [[328, 360], [328, 358], [262, 283], [162, 283], [67, 303], [56, 303], [44, 303], [44, 246], [60, 133]]\n",
            "\n",
            "Total Timesteps: 8587 Episode Num: 16 Episode Len: 720 Reward: -469.5 Time Taken: 11.422050714492798 secs\n",
            "Rewards Distribution:  [('road (+1)', 89), ('road (-0.1)', 85), ('sand (-1)', 545), ('wall (-5)', 1)]\n",
            "Training took 30.43443989753723 secs\n",
            "Path [[570, 309], [571, 310], [588, 410], [602, 373], [596, 262], [592, 171], [592, 168], [592, 149], [622, 38]]\n",
            "\n",
            "Total Timesteps: 8701 Episode Num: 17 Episode Len: 114 Reward: -12.200000000000003 Time Taken: 1.8788559436798096 secs\n",
            "Rewards Distribution:  [('road (+1)', 52), ('road (-0.1)', 2), ('sand (-1)', 59), ('wall (-5)', 1)]\n",
            "Training took 4.804268836975098 secs\n",
            "Path [[848, 100], [849, 101], [891, 37]]\n",
            "\n",
            "Total Timesteps: 10246 Episode Num: 18 Episode Len: 1545 Reward: -1222.5000000000018 Time Taken: 24.952298164367676 secs\n",
            "Rewards Distribution:  [('road (+1)', 62), ('road (-0.1)', 225), ('sand (-1)', 1257), ('wall (-5)', 1)]\n",
            "Training took 65.27682423591614 secs\n",
            "Path [[1087, 319], [1085, 317], [971, 233], [861, 219], [772, 232], [752, 313], [649, 318], [609, 321], [638, 233], [528, 147], [430, 162], [335, 200], [238, 204], [152, 219], [138, 221], [95, 157], [45, 109]]\n",
            "\n",
            "Total Timesteps: 10657 Episode Num: 19 Episode Len: 411 Reward: -293.30000000000024 Time Taken: 7.273444414138794 secs\n",
            "Rewards Distribution:  [('road (+1)', 46), ('road (-0.1)', 33), ('sand (-1)', 331), ('wall (-5)', 1)]\n",
            "Training took 17.371181964874268 secs\n",
            "Path [[755, 225], [753, 223], [705, 171], [657, 112], [602, 61], [566, 32]]\n",
            "\n",
            "Total Timesteps: 10883 Episode Num: 20 Episode Len: 226 Reward: -211.1 Time Taken: 4.017855644226074 secs\n",
            "Rewards Distribution:  [('road (-0.1)', 21), ('sand (-1)', 204), ('wall (-5)', 1)]\n",
            "Training took 9.533286094665527 secs\n",
            "Path [[146, 385], [146, 386], [80, 334], [26, 289]]\n",
            "\n",
            "Total Timesteps: 11399 Episode Num: 21 Episode Len: 516 Reward: -463.4000000000008 Time Taken: 9.253921270370483 secs\n",
            "Rewards Distribution:  [('road (+1)', 13), ('road (-0.1)', 34), ('sand (-1)', 468), ('wall (-5)', 1)]\n",
            "Training took 21.79664707183838 secs\n",
            "Path [[1286, 314], [1285, 312], [1242, 259], [1196, 205], [1137, 142], [1082, 96], [1029, 44]]\n",
            "\n",
            "Total Timesteps: 11668 Episode Num: 22 Episode Len: 269 Reward: -132.59999999999985 Time Taken: 4.8047990798950195 secs\n",
            "Rewards Distribution:  [('road (+1)', 45), ('road (-0.1)', 56), ('sand (-1)', 167), ('wall (-5)', 1)]\n",
            "Training took 11.384509563446045 secs\n",
            "Path [[589, 148], [587, 149], [528, 94], [482, 40]]\n",
            "\n",
            "Total Timesteps: 12124 Episode Num: 23 Episode Len: 456 Reward: -393.5999999999998 Time Taken: 8.078792572021484 secs\n",
            "Rewards Distribution:  [('road (+1)', 17), ('road (-0.1)', 36), ('sand (-1)', 402), ('wall (-5)', 1)]\n",
            "Training took 19.25644540786743 secs\n",
            "Path [[249, 529], [248, 527], [203, 480], [156, 421], [92, 371], [38, 325]]\n",
            "\n",
            "Total Timesteps: 13135 Episode Num: 24 Episode Len: 1011 Reward: -744.8000000000009 Time Taken: 18.016123294830322 secs\n",
            "Rewards Distribution:  [('road (+1)', 82), ('road (-0.1)', 118), ('sand (-1)', 810), ('wall (-5)', 1)]\n",
            "Training took 42.71915531158447 secs\n",
            "Path [[860, 579], [861, 577], [804, 521], [758, 471], [709, 415], [659, 350], [606, 310], [557, 250], [510, 196], [457, 131], [402, 81], [360, 29]]\n",
            "\n",
            "Total Timesteps: 13366 Episode Num: 25 Episode Len: 231 Reward: -188.99999999999991 Time Taken: 4.138662099838257 secs\n",
            "Rewards Distribution:  [('road (+1)', 14), ('road (-0.1)', 20), ('sand (-1)', 196), ('wall (-5)', 1)]\n",
            "Training took 9.79220700263977 secs\n",
            "Path [[451, 140], [449, 141], [405, 87], [342, 31]]\n",
            "\n",
            "Total Timesteps: 13433 Episode Num: 26 Episode Len: 67 Reward: -25.200000000000003 Time Taken: 1.2173216342926025 secs\n",
            "Rewards Distribution:  [('road (+1)', 13), ('road (-0.1)', 22), ('sand (-1)', 31), ('wall (-5)', 1)]\n",
            "Training took 2.8256676197052 secs\n",
            "Path [[1138, 54], [1139, 52]]\n",
            "\n",
            "Total Timesteps: 13437 Episode Num: 27 Episode Len: 4 Reward: -3.1 Time Taken: 0.09243607521057129 secs\n",
            "Rewards Distribution:  [('road (+1)', 2), ('road (-0.1)', 1), ('wall (-5)', 1)]\n",
            "Training took 0.15998053550720215 secs\n",
            "Path [[1064, 26], [1065, 24]]\n",
            "\n",
            "Total Timesteps: 13696 Episode Num: 28 Episode Len: 259 Reward: -231.4 Time Taken: 4.57489538192749 secs\n",
            "Rewards Distribution:  [('road (+1)', 5), ('road (-0.1)', 24), ('sand (-1)', 229), ('wall (-5)', 1)]\n",
            "Training took 10.94414234161377 secs\n",
            "Path [[166, 385], [164, 386], [99, 322], [44, 273]]\n",
            "\n",
            "Total Timesteps: 13789 Episode Num: 29 Episode Len: 93 Reward: -42.00000000000001 Time Taken: 1.7177839279174805 secs\n",
            "Rewards Distribution:  [('road (+1)', 23), ('road (-0.1)', 10), ('sand (-1)', 59), ('wall (-5)', 1)]\n",
            "Training took 3.9280128479003906 secs\n",
            "Path [[588, 44], [588, 45]]\n",
            "\n",
            "Total Timesteps: 14989 Episode Num: 30 Episode Len: 1200 Reward: -1048.2000000000012 Time Taken: 21.13502597808838 secs\n",
            "Rewards Distribution:  [('road (+1)', 41), ('road (-0.1)', 82), ('sand (-1)', 1076), ('wall (-5)', 1)]\n",
            "Training took 50.734734535217285 secs\n",
            "Path [[629, 638], [630, 638], [579, 590], [522, 538], [479, 485], [426, 443], [374, 379], [328, 336], [273, 290], [223, 232], [174, 183], [114, 129], [68, 68]]\n",
            "\n",
            "Total Timesteps: 15432 Episode Num: 31 Episode Len: 443 Reward: -305.6999999999997 Time Taken: 7.981770753860474 secs\n",
            "Rewards Distribution:  [('road (+1)', 45), ('road (-0.1)', 57), ('sand (-1)', 340), ('wall (-5)', 1)]\n",
            "Training took 18.73629331588745 secs\n",
            "Path [[652, 246], [652, 247], [616, 191], [560, 132], [509, 89], [455, 38]]\n",
            "\n",
            "Total Timesteps: 16012 Episode Num: 32 Episode Len: 580 Reward: -471.60000000000036 Time Taken: 10.244586706161499 secs\n",
            "Rewards Distribution:  [('road (+1)', 31), ('road (-0.1)', 56), ('sand (-1)', 492), ('wall (-5)', 1)]\n",
            "Training took 24.485084533691406 secs\n",
            "Path [[367, 318], [367, 316], [303, 264], [248, 209], [202, 155], [155, 108], [102, 62]]\n",
            "\n",
            "Total Timesteps: 16456 Episode Num: 33 Episode Len: 444 Reward: -276.59999999999957 Time Taken: 7.856409072875977 secs\n",
            "Rewards Distribution:  [('road (+1)', 47), ('road (-0.1)', 86), ('sand (-1)', 310), ('wall (-5)', 1)]\n",
            "Training took 18.75864291191101 secs\n",
            "Path [[883, 232], [884, 233], [820, 182], [770, 131], [723, 90], [665, 41]]\n",
            "\n",
            "Total Timesteps: 17279 Episode Num: 34 Episode Len: 823 Reward: -671.9000000000007 Time Taken: 14.569914102554321 secs\n",
            "Rewards Distribution:  [('road (+1)', 42), ('road (-0.1)', 79), ('sand (-1)', 701), ('wall (-5)', 1)]\n",
            "Training took 34.74825954437256 secs\n",
            "Path [[697, 446], [695, 446], [642, 401], [595, 363], [527, 301], [477, 246], [431, 195], [384, 138], [322, 96], [277, 35]]\n",
            "\n",
            "Total Timesteps: 17290 Episode Num: 35 Episode Len: 11 Reward: 0.6000000000000014 Time Taken: 0.215651273727417 secs\n",
            "Rewards Distribution:  [('road (+1)', 6), ('road (-0.1)', 4), ('wall (-5)', 1)]\n",
            "Training took 0.460949182510376 secs\n",
            "Path [[1318, 38], [1319, 37]]\n",
            "\n",
            "Total Timesteps: 17589 Episode Num: 36 Episode Len: 299 Reward: -217.8999999999999 Time Taken: 5.3197760581970215 secs\n",
            "Rewards Distribution:  [('road (+1)', 25), ('road (-0.1)', 39), ('sand (-1)', 234), ('wall (-5)', 1)]\n",
            "Training took 12.646061182022095 secs\n",
            "Path [[586, 185], [587, 185], [532, 114], [483, 60]]\n",
            "\n",
            "Total Timesteps: 17619 Episode Num: 37 Episode Len: 30 Reward: -9.399999999999999 Time Taken: 0.5576090812683105 secs\n",
            "Rewards Distribution:  [('road (+1)', 6), ('road (-0.1)', 14), ('sand (-1)', 9), ('wall (-5)', 1)]\n",
            "Training took 1.2566606998443604 secs\n",
            "Path [[1395, 35], [1393, 35]]\n",
            "\n",
            "Total Timesteps: 17831 Episode Num: 38 Episode Len: 212 Reward: -216 Time Taken: 3.806074857711792 secs\n",
            "Rewards Distribution:  [('sand (-1)', 211), ('wall (-5)', 1)]\n",
            "Training took 8.943065166473389 secs\n",
            "Path [[124, 354], [122, 353], [76, 307], [21, 261]]\n",
            "\n",
            "Total Timesteps: 18912 Episode Num: 39 Episode Len: 1081 Reward: -965.3000000000012 Time Taken: 19.092336893081665 secs\n",
            "Rewards Distribution:  [('road (+1)', 36), ('road (-0.1)', 53), ('sand (-1)', 991), ('wall (-5)', 1)]\n",
            "Training took 45.657373666763306 secs\n",
            "Path [[592, 571], [590, 572], [545, 522], [495, 476], [440, 426], [389, 371], [337, 320], [288, 275], [234, 227], [182, 162], [137, 108], [91, 62]]\n",
            "\n",
            "Total Timesteps: 19183 Episode Num: 40 Episode Len: 271 Reward: -224.49999999999986 Time Taken: 4.80790376663208 secs\n",
            "Rewards Distribution:  [('road (+1)', 14), ('road (-0.1)', 25), ('sand (-1)', 231), ('wall (-5)', 1)]\n",
            "Training took 11.470361471176147 secs\n",
            "Path [[411, 167], [410, 168], [368, 104], [317, 51]]\n",
            "\n",
            "Total Timesteps: 19663 Episode Num: 41 Episode Len: 480 Reward: -335.7999999999997 Time Taken: 8.488015174865723 secs\n",
            "Rewards Distribution:  [('road (+1)', 39), ('road (-0.1)', 78), ('sand (-1)', 362), ('wall (-5)', 1)]\n",
            "Training took 20.264697074890137 secs\n",
            "Path [[948, 255], [949, 253], [894, 203], [843, 155], [803, 113], [745, 69]]\n",
            "\n",
            "Total Timesteps: 20158 Episode Num: 42 Episode Len: 495 Reward: -343.50000000000034 Time Taken: 8.77499270439148 secs\n",
            "Rewards Distribution:  [('road (+1)', 44), ('road (-0.1)', 75), ('sand (-1)', 375), ('wall (-5)', 1)]\n",
            "Training took 20.928722858428955 secs\n",
            "Path [[1342, 272], [1340, 273], [1305, 222], [1255, 177], [1202, 128], [1140, 72]]\n",
            "\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -298.300000\n",
            "---------------------------------------\n",
            "Total Timesteps: 20283 Episode Num: 43 Episode Len: 125 Reward: -78.10000000000002 Time Taken: 2.2542335987091064 secs\n",
            "Rewards Distribution:  [('road (+1)', 285), ('road (-0.1)', 481), ('sand (-1)', 3243), ('wall (-5)', 11)]\n",
            "Training took 5.29875111579895 secs\n",
            "Path [[862, 90], [863, 91], [815, 32]]\n",
            "\n",
            "Total Timesteps: 20494 Episode Num: 44 Episode Len: 211 Reward: -144.3999999999999 Time Taken: 3.853261947631836 secs\n",
            "Rewards Distribution:  [('road (+1)', 20), ('road (-0.1)', 34), ('sand (-1)', 156), ('wall (-5)', 1)]\n",
            "Training took 8.899264335632324 secs\n",
            "Path [[127, 301], [128, 302], [77, 245], [29, 193]]\n",
            "\n",
            "Total Timesteps: 21003 Episode Num: 45 Episode Len: 509 Reward: -376.8000000000003 Time Taken: 9.323814153671265 secs\n",
            "Rewards Distribution:  [('road (+1)', 42), ('road (-0.1)', 58), ('sand (-1)', 408), ('wall (-5)', 1)]\n",
            "Training took 21.438918352127075 secs\n",
            "Path [[780, 271], [778, 269], [716, 218], [662, 169], [605, 113], [572, 74], [533, 34]]\n",
            "\n",
            "Total Timesteps: 21436 Episode Num: 46 Episode Len: 433 Reward: -363.0000000000004 Time Taken: 7.80892276763916 secs\n",
            "Rewards Distribution:  [('road (+1)', 28), ('road (-0.1)', 20), ('sand (-1)', 384), ('wall (-5)', 1)]\n",
            "Training took 18.313473224639893 secs\n",
            "Path [[998, 255], [998, 256], [966, 198], [918, 155], [862, 105], [823, 32]]\n",
            "\n",
            "Total Timesteps: 22144 Episode Num: 47 Episode Len: 708 Reward: -555.1000000000001 Time Taken: 12.561346054077148 secs\n",
            "Rewards Distribution:  [('road (+1)', 51), ('road (-0.1)', 61), ('sand (-1)', 595), ('wall (-5)', 1)]\n",
            "Training took 29.931806087493896 secs\n",
            "Path [[649, 404], [650, 405], [602, 340], [562, 295], [514, 249], [460, 202], [410, 136], [361, 73], [314, 27]]\n",
            "\n",
            "Total Timesteps: 22565 Episode Num: 48 Episode Len: 421 Reward: -311.9999999999998 Time Taken: 7.504249811172485 secs\n",
            "Rewards Distribution:  [('road (+1)', 34), ('road (-0.1)', 50), ('sand (-1)', 336), ('wall (-5)', 1)]\n",
            "Training took 17.808292627334595 secs\n",
            "Path [[240, 543], [241, 541], [186, 483], [132, 429], [85, 379], [37, 333]]\n",
            "\n",
            "Total Timesteps: 23318 Episode Num: 49 Episode Len: 753 Reward: -625.200000000001 Time Taken: 13.2731032371521 secs\n",
            "Rewards Distribution:  [('road (+1)', 38), ('road (-0.1)', 62), ('sand (-1)', 652), ('wall (-5)', 1)]\n",
            "Training took 31.815775156021118 secs\n",
            "Path [[406, 601], [404, 601], [357, 553], [306, 507], [252, 457], [201, 402], [150, 348], [97, 311], [43, 263]]\n",
            "\n",
            "Total Timesteps: 23812 Episode Num: 50 Episode Len: 494 Reward: -371.80000000000035 Time Taken: 8.920109510421753 secs\n",
            "Rewards Distribution:  [('road (+1)', 37), ('road (-0.1)', 58), ('sand (-1)', 398), ('wall (-5)', 1)]\n",
            "Training took 20.89820671081543 secs\n",
            "Path [[1095, 279], [1093, 279], [1048, 233], [996, 187], [942, 136], [894, 71]]\n",
            "\n",
            "Total Timesteps: 24510 Episode Num: 51 Episode Len: 698 Reward: -458.2000000000021 Time Taken: 12.725344181060791 secs\n",
            "Rewards Distribution:  [('road (+1)', 67), ('road (-0.1)', 122), ('sand (-1)', 508), ('wall (-5)', 1)]\n",
            "Training took 29.526777267456055 secs\n",
            "Path [[1055, 364], [1053, 362], [998, 316], [930, 271], [893, 211], [840, 165], [785, 127], [723, 82]]\n",
            "\n",
            "Total Timesteps: 24707 Episode Num: 52 Episode Len: 197 Reward: -192.5 Time Taken: 3.569087266921997 secs\n",
            "Rewards Distribution:  [('road (+1)', 2), ('road (-0.1)', 5), ('sand (-1)', 189), ('wall (-5)', 1)]\n",
            "Training took 8.35292387008667 secs\n",
            "Path [[117, 562], [117, 560], [66, 501]]\n",
            "\n",
            "Total Timesteps: 24875 Episode Num: 53 Episode Len: 168 Reward: -107.8999999999998 Time Taken: 3.062748908996582 secs\n",
            "Rewards Distribution:  [('road (+1)', 10), ('road (-0.1)', 49), ('sand (-1)', 108), ('wall (-5)', 1)]\n",
            "Training took 7.097923994064331 secs\n",
            "Path [[673, 140], [673, 141], [626, 88]]\n",
            "\n",
            "Total Timesteps: 25017 Episode Num: 54 Episode Len: 142 Reward: -140.6 Time Taken: 2.552321672439575 secs\n",
            "Rewards Distribution:  [('road (-0.1)', 6), ('sand (-1)', 135), ('wall (-5)', 1)]\n",
            "Training took 6.004025220870972 secs\n",
            "Path [[96, 397], [95, 395], [37, 356]]\n",
            "\n",
            "Total Timesteps: 25177 Episode Num: 55 Episode Len: 160 Reward: -93.1 Time Taken: 2.8619370460510254 secs\n",
            "Rewards Distribution:  [('road (+1)', 35), ('road (-0.1)', 1), ('sand (-1)', 123), ('wall (-5)', 1)]\n",
            "Training took 6.758026599884033 secs\n",
            "Path [[335, 216], [335, 214], [346, 79]]\n",
            "\n",
            "Total Timesteps: 25563 Episode Num: 56 Episode Len: 386 Reward: -168.19999999999953 Time Taken: 6.952279329299927 secs\n",
            "Rewards Distribution:  [('road (+1)', 74), ('road (-0.1)', 82), ('sand (-1)', 229), ('wall (-5)', 1)]\n",
            "Training took 16.307052850723267 secs\n",
            "Path [[1217, 149], [1217, 147], [1226, 41], [1286, 42], [1203, 53]]\n",
            "\n",
            "Total Timesteps: 25913 Episode Num: 57 Episode Len: 350 Reward: -331.50000000000045 Time Taken: 6.267465353012085 secs\n",
            "Rewards Distribution:  [('road (-0.1)', 25), ('sand (-1)', 324), ('wall (-5)', 1)]\n",
            "Training took 14.808796167373657 secs\n",
            "Path [[160, 553], [159, 551], [131, 445], [131, 406], [87, 406]]\n",
            "\n",
            "Total Timesteps: 26999 Episode Num: 58 Episode Len: 1086 Reward: -966.3000000000012 Time Taken: 19.80247950553894 secs\n",
            "Rewards Distribution:  [('road (+1)', 20), ('road (-0.1)', 93), ('sand (-1)', 972), ('wall (-5)', 1)]\n",
            "Training took 45.88128685951233 secs\n",
            "Path [[787, 603], [788, 603], [734, 567], [658, 509], [620, 452], [557, 413], [498, 355], [452, 295], [397, 258], [336, 175], [304, 122], [235, 71]]\n",
            "\n",
            "Total Timesteps: 28089 Episode Num: 59 Episode Len: 1090 Reward: -882.6000000000017 Time Taken: 19.454352855682373 secs\n",
            "Rewards Distribution:  [('road (+1)', 67), ('road (-0.1)', 86), ('sand (-1)', 936), ('wall (-5)', 1)]\n",
            "Training took 46.0472207069397 secs\n",
            "Path [[604, 568], [604, 566], [557, 519], [507, 463], [452, 417], [409, 369], [358, 295], [304, 264], [261, 213], [212, 157], [164, 119], [117, 60]]\n",
            "\n",
            "Total Timesteps: 28831 Episode Num: 60 Episode Len: 742 Reward: -609.2000000000008 Time Taken: 13.321166753768921 secs\n",
            "Rewards Distribution:  [('road (+1)', 27), ('road (-0.1)', 92), ('sand (-1)', 622), ('wall (-5)', 1)]\n",
            "Training took 31.37041997909546 secs\n",
            "Path [[1177, 378], [1178, 378], [1148, 418], [1046, 335], [1046, 284], [951, 260], [907, 157], [873, 154], [774, 71]]\n",
            "\n",
            "Total Timesteps: 29615 Episode Num: 61 Episode Len: 784 Reward: -505.70000000000084 Time Taken: 13.893494129180908 secs\n",
            "Rewards Distribution:  [('road (+1)', 57), ('road (-0.1)', 187), ('sand (-1)', 539), ('wall (-5)', 1)]\n",
            "Training took 33.1387894153595 secs\n",
            "Path [[1312, 287], [1313, 286], [1366, 267], [1366, 267], [1315, 303], [1151, 387], [1036, 358], [927, 249], [839, 125]]\n",
            "\n",
            "Total Timesteps: 30100 Episode Num: 62 Episode Len: 485 Reward: -299.59999999999957 Time Taken: 8.591078281402588 secs\n",
            "Rewards Distribution:  [('road (+1)', 56), ('road (-0.1)', 86), ('sand (-1)', 342), ('wall (-5)', 1)]\n",
            "Training took 20.516116857528687 secs\n",
            "Path [[887, 261], [887, 262], [836, 206], [784, 153], [734, 119], [680, 60]]\n",
            "\n",
            "Total Timesteps: 30224 Episode Num: 63 Episode Len: 124 Reward: -107.3 Time Taken: 2.246194362640381 secs\n",
            "Rewards Distribution:  [('road (+1)', 9), ('road (-0.1)', 3), ('sand (-1)', 111), ('wall (-5)', 1)]\n",
            "Training took 5.228816747665405 secs\n",
            "Path [[72, 405], [73, 405], [24, 342]]\n",
            "\n",
            "Total Timesteps: 31312 Episode Num: 64 Episode Len: 1088 Reward: -805.200000000002 Time Taken: 19.982701063156128 secs\n",
            "Rewards Distribution:  [('road (+1)', 93), ('road (-0.1)', 112), ('sand (-1)', 882), ('wall (-5)', 1)]\n",
            "Training took 45.95079708099365 secs\n",
            "Path [[905, 588], [906, 587], [854, 526], [798, 480], [754, 422], [705, 367], [646, 328], [605, 269], [557, 217], [509, 171], [455, 122], [407, 59]]\n",
            "\n",
            "Total Timesteps: 32234 Episode Num: 65 Episode Len: 922 Reward: -677.6000000000008 Time Taken: 16.46200704574585 secs\n",
            "Rewards Distribution:  [('road (+1)', 72), ('road (-0.1)', 116), ('sand (-1)', 733), ('wall (-5)', 1)]\n",
            "Training took 38.96560597419739 secs\n",
            "Path [[826, 502], [826, 503], [779, 451], [734, 403], [679, 352], [619, 292], [568, 238], [523, 189], [467, 149], [412, 92], [361, 28]]\n",
            "\n",
            "Total Timesteps: 32936 Episode Num: 66 Episode Len: 702 Reward: -615.3000000000005 Time Taken: 12.912517547607422 secs\n",
            "Rewards Distribution:  [('road (+1)', 26), ('road (-0.1)', 43), ('sand (-1)', 632), ('wall (-5)', 1)]\n",
            "Training took 29.681797981262207 secs\n",
            "Path [[379, 424], [380, 423], [315, 371], [269, 307], [224, 257], [179, 208], [123, 152], [66, 97], [20, 45]]\n",
            "\n",
            "Total Timesteps: 33627 Episode Num: 67 Episode Len: 691 Reward: -588.4000000000011 Time Taken: 12.221519947052002 secs\n",
            "Rewards Distribution:  [('road (+1)', 29), ('road (-0.1)', 54), ('sand (-1)', 607), ('wall (-5)', 1)]\n",
            "Training took 29.19465732574463 secs\n",
            "Path [[955, 383], [956, 381], [903, 330], [854, 264], [808, 213], [768, 178], [694, 137], [648, 62]]\n",
            "\n",
            "Total Timesteps: 34433 Episode Num: 68 Episode Len: 806 Reward: -506.2000000000009 Time Taken: 14.354623317718506 secs\n",
            "Rewards Distribution:  [('road (+1)', 106), ('road (-0.1)', 102), ('sand (-1)', 597), ('wall (-5)', 1)]\n",
            "Training took 34.068273067474365 secs\n",
            "Path [[350, 624], [351, 624], [314, 593], [296, 555], [268, 523], [216, 477], [162, 425], [116, 370], [70, 321], [20, 275]]\n",
            "\n",
            "Total Timesteps: 34767 Episode Num: 69 Episode Len: 334 Reward: -252.7000000000004 Time Taken: 6.25494909286499 secs\n",
            "Rewards Distribution:  [('road (+1)', 26), ('road (-0.1)', 37), ('sand (-1)', 270), ('wall (-5)', 1)]\n",
            "Training took 14.152621746063232 secs\n",
            "Path [[1406, 211], [1407, 209], [1354, 153], [1301, 87], [1251, 28]]\n",
            "\n",
            "Total Timesteps: 34824 Episode Num: 70 Episode Len: 57 Reward: -31.500000000000007 Time Taken: 1.0256950855255127 secs\n",
            "Rewards Distribution:  [('road (+1)', 8), ('road (-0.1)', 15), ('sand (-1)', 33), ('wall (-5)', 1)]\n",
            "Training took 2.4078495502471924 secs\n",
            "Path [[1070, 66], [1070, 67]]\n",
            "\n",
            "Total Timesteps: 35381 Episode Num: 71 Episode Len: 557 Reward: -402.60000000000014 Time Taken: 9.809546709060669 secs\n",
            "Rewards Distribution:  [('road (+1)', 45), ('road (-0.1)', 76), ('sand (-1)', 435), ('wall (-5)', 1)]\n",
            "Training took 23.544933557510376 secs\n",
            "Path [[824, 297], [824, 298], [781, 236], [736, 192], [687, 147], [627, 95], [568, 38]]\n",
            "\n",
            "Total Timesteps: 36516 Episode Num: 72 Episode Len: 1135 Reward: -911.400000000002 Time Taken: 19.972774744033813 secs\n",
            "Rewards Distribution:  [('road (+1)', 58), ('road (-0.1)', 124), ('sand (-1)', 952), ('wall (-5)', 1)]\n",
            "Training took 47.95550608634949 secs\n",
            "Path [[801, 597], [802, 597], [754, 537], [696, 482], [638, 431], [575, 389], [521, 337], [472, 283], [426, 233], [376, 187], [322, 138], [267, 84], [213, 35]]\n",
            "\n",
            "Total Timesteps: 36546 Episode Num: 73 Episode Len: 30 Reward: 9.700000000000001 Time Taken: 0.5714964866638184 secs\n",
            "Rewards Distribution:  [('road (+1)', 16), ('road (-0.1)', 13), ('wall (-5)', 1)]\n",
            "Training took 1.2600467205047607 secs\n",
            "Path [[1023, 27], [1021, 26]]\n",
            "\n",
            "Total Timesteps: 36846 Episode Num: 74 Episode Len: 300 Reward: -195.39999999999995 Time Taken: 5.446791172027588 secs\n",
            "Rewards Distribution:  [('road (+1)', 30), ('road (-0.1)', 54), ('sand (-1)', 215), ('wall (-5)', 1)]\n",
            "Training took 12.695040225982666 secs\n",
            "Path [[810, 165], [809, 166], [757, 114], [715, 67]]\n",
            "\n",
            "Total Timesteps: 36893 Episode Num: 75 Episode Len: 47 Reward: 9.000000000000002 Time Taken: 0.8546805381774902 secs\n",
            "Rewards Distribution:  [('road (+1)', 21), ('road (-0.1)', 20), ('sand (-1)', 5), ('wall (-5)', 1)]\n",
            "Training took 1.9850690364837646 secs\n",
            "Path [[491, 48], [492, 48]]\n",
            "\n",
            "Total Timesteps: 37120 Episode Num: 76 Episode Len: 227 Reward: -195.3999999999999 Time Taken: 4.002262115478516 secs\n",
            "Rewards Distribution:  [('road (+1)', 7), ('road (-0.1)', 24), ('sand (-1)', 195), ('wall (-5)', 1)]\n",
            "Training took 9.588303804397583 secs\n",
            "Path [[1155, 166], [1155, 164], [1097, 109], [1045, 44]]\n",
            "\n",
            "Total Timesteps: 37227 Episode Num: 77 Episode Len: 107 Reward: -57.60000000000001 Time Taken: 1.9018137454986572 secs\n",
            "Rewards Distribution:  [('road (+1)', 15), ('road (-0.1)', 26), ('sand (-1)', 65), ('wall (-5)', 1)]\n",
            "Training took 4.521171569824219 secs\n",
            "Path [[985, 77], [983, 76], [939, 30]]\n",
            "\n",
            "Total Timesteps: 37804 Episode Num: 78 Episode Len: 577 Reward: -482.60000000000053 Time Taken: 10.14602518081665 secs\n",
            "Rewards Distribution:  [('road (+1)', 33), ('road (-0.1)', 36), ('sand (-1)', 507), ('wall (-5)', 1)]\n",
            "Training took 24.39974308013916 secs\n",
            "Path [[304, 358], [302, 358], [262, 317], [209, 271], [154, 219], [110, 155], [59, 105]]\n",
            "\n",
            "Total Timesteps: 38720 Episode Num: 79 Episode Len: 916 Reward: -670.2000000000008 Time Taken: 16.278014421463013 secs\n",
            "Rewards Distribution:  [('road (+1)', 70), ('road (-0.1)', 122), ('sand (-1)', 723), ('wall (-5)', 1)]\n",
            "Training took 38.67058300971985 secs\n",
            "Path [[727, 510], [728, 509], [656, 453], [607, 387], [558, 337], [509, 290], [455, 242], [403, 188], [352, 133], [305, 88], [251, 41]]\n",
            "\n",
            "Total Timesteps: 39709 Episode Num: 80 Episode Len: 989 Reward: -769.0000000000013 Time Taken: 17.493982315063477 secs\n",
            "Rewards Distribution:  [('road (+1)', 67), ('road (-0.1)', 100), ('sand (-1)', 821), ('wall (-5)', 1)]\n",
            "Training took 41.75421667098999 secs\n",
            "Path [[682, 505], [682, 506], [635, 455], [595, 408], [535, 362], [485, 303], [434, 248], [388, 197], [335, 152], [272, 113], [221, 60]]\n",
            "\n",
            "Total Timesteps: 39774 Episode Num: 81 Episode Len: 65 Reward: -56.1 Time Taken: 1.1676833629608154 secs\n",
            "Rewards Distribution:  [('road (+1)', 6), ('road (-0.1)', 1), ('sand (-1)', 57), ('wall (-5)', 1)]\n",
            "Training took 2.7481884956359863 secs\n",
            "Path [[46, 396], [47, 396]]\n",
            "\n",
            "Total Timesteps: 39954 Episode Num: 82 Episode Len: 180 Reward: -134.9 Time Taken: 3.2615714073181152 secs\n",
            "Rewards Distribution:  [('road (+1)', 16), ('road (-0.1)', 19), ('sand (-1)', 144), ('wall (-5)', 1)]\n",
            "Training took 7.638733625411987 secs\n",
            "Path [[109, 400], [108, 398], [57, 353]]\n",
            "\n",
            "Total Timesteps: 40524 Episode Num: 83 Episode Len: 570 Reward: -446.3000000000006 Time Taken: 9.980778455734253 secs\n",
            "Rewards Distribution:  [('road (+1)', 40), ('road (-0.1)', 53), ('sand (-1)', 476), ('wall (-5)', 1)]\n",
            "Training took 24.06848931312561 secs\n",
            "Path [[513, 314], [512, 315], [479, 260], [433, 214], [373, 178], [321, 115], [273, 61]]\n",
            "\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -204.320000\n",
            "---------------------------------------\n",
            "Total Timesteps: 41602 Episode Num: 84 Episode Len: 1078 Reward: -859.4000000000012 Time Taken: 18.870800971984863 secs\n",
            "Rewards Distribution:  [('road (+1)', 359), ('road (-0.1)', 556), ('sand (-1)', 3151), ('wall (-5)', 11)]\n",
            "Training took 45.50235342979431 secs\n",
            "Path [[733, 554], [732, 552], [676, 519], [621, 459], [562, 406], [516, 355], [471, 300], [417, 252], [364, 198], [319, 145], [270, 99], [208, 60]]\n",
            "\n",
            "Total Timesteps: 42079 Episode Num: 85 Episode Len: 477 Reward: -387.5000000000005 Time Taken: 8.319019794464111 secs\n",
            "Rewards Distribution:  [('road (+1)', 22), ('road (-0.1)', 55), ('sand (-1)', 399), ('wall (-5)', 1)]\n",
            "Training took 20.157310485839844 secs\n",
            "Path [[1143, 286], [1144, 286], [1090, 216], [1035, 167], [989, 121], [933, 65]]\n",
            "\n",
            "Total Timesteps: 42106 Episode Num: 86 Episode Len: 27 Reward: 10.000000000000002 Time Taken: 0.4910762310028076 secs\n",
            "Rewards Distribution:  [('road (+1)', 16), ('road (-0.1)', 10), ('wall (-5)', 1)]\n",
            "Training took 1.1377670764923096 secs\n",
            "Path [[1396, 25], [1394, 26]]\n",
            "\n",
            "Total Timesteps: 42210 Episode Num: 87 Episode Len: 104 Reward: -80.1 Time Taken: 1.8731443881988525 secs\n",
            "Rewards Distribution:  [('road (+1)', 9), ('road (-0.1)', 11), ('sand (-1)', 83), ('wall (-5)', 1)]\n",
            "Training took 4.384986639022827 secs\n",
            "Path [[68, 225], [66, 226], [22, 174]]\n",
            "\n",
            "Total Timesteps: 42609 Episode Num: 88 Episode Len: 399 Reward: -322.00000000000034 Time Taken: 7.021060228347778 secs\n",
            "Rewards Distribution:  [('road (+1)', 18), ('road (-0.1)', 50), ('sand (-1)', 330), ('wall (-5)', 1)]\n",
            "Training took 16.852864980697632 secs\n",
            "Path [[1151, 222], [1149, 220], [1093, 173], [1031, 130], [967, 77]]\n",
            "\n",
            "Total Timesteps: 43075 Episode Num: 89 Episode Len: 466 Reward: -347.70000000000084 Time Taken: 8.148290157318115 secs\n",
            "Rewards Distribution:  [('road (+1)', 31), ('road (-0.1)', 67), ('sand (-1)', 367), ('wall (-5)', 1)]\n",
            "Training took 19.659902811050415 secs\n",
            "Path [[1360, 260], [1358, 258], [1304, 210], [1249, 160], [1198, 106], [1143, 56]]\n",
            "\n",
            "Total Timesteps: 43492 Episode Num: 90 Episode Len: 417 Reward: -288.5000000000002 Time Taken: 7.3273866176605225 secs\n",
            "Rewards Distribution:  [('road (+1)', 37), ('road (-0.1)', 65), ('sand (-1)', 314), ('wall (-5)', 1)]\n",
            "Training took 17.602078914642334 secs\n",
            "Path [[668, 246], [669, 247], [628, 187], [583, 140], [529, 89], [464, 33]]\n",
            "\n",
            "Total Timesteps: 43618 Episode Num: 91 Episode Len: 126 Reward: -45.400000000000034 Time Taken: 2.2198922634124756 secs\n",
            "Rewards Distribution:  [('road (+1)', 27), ('road (-0.1)', 34), ('sand (-1)', 64), ('wall (-5)', 1)]\n",
            "Training took 5.3124260902404785 secs\n",
            "Path [[1334, 78], [1334, 79], [1277, 22]]\n",
            "\n",
            "Total Timesteps: 44246 Episode Num: 92 Episode Len: 628 Reward: -539.7000000000005 Time Taken: 11.105741024017334 secs\n",
            "Rewards Distribution:  [('road (+1)', 25), ('road (-0.1)', 47), ('sand (-1)', 555), ('wall (-5)', 1)]\n",
            "Training took 26.50964593887329 secs\n",
            "Path [[379, 349], [380, 348], [325, 298], [277, 244], [234, 187], [180, 138], [121, 87], [71, 33]]\n",
            "\n",
            "Total Timesteps: 44253 Episode Num: 93 Episode Len: 7 Reward: 1 Time Taken: 0.14243721961975098 secs\n",
            "Rewards Distribution:  [('road (+1)', 6), ('wall (-5)', 1)]\n",
            "Training took 0.29293012619018555 secs\n",
            "Path [[1346, 24], [1347, 24]]\n",
            "\n",
            "Total Timesteps: 44342 Episode Num: 94 Episode Len: 89 Reward: -29.800000000000026 Time Taken: 1.617150068283081 secs\n",
            "Rewards Distribution:  [('road (+1)', 19), ('road (-0.1)', 28), ('sand (-1)', 41), ('wall (-5)', 1)]\n",
            "Training took 3.754410982131958 secs\n",
            "Path [[1018, 71], [1018, 72]]\n",
            "\n",
            "Total Timesteps: 44771 Episode Num: 95 Episode Len: 429 Reward: -380.70000000000016 Time Taken: 7.505638360977173 secs\n",
            "Rewards Distribution:  [('road (+1)', 14), ('road (-0.1)', 27), ('sand (-1)', 387), ('wall (-5)', 1)]\n",
            "Training took 18.121368408203125 secs\n",
            "Path [[245, 382], [243, 383], [201, 326], [153, 280], [88, 232], [32, 167]]\n",
            "\n",
            "Total Timesteps: 44894 Episode Num: 96 Episode Len: 123 Reward: -90.5 Time Taken: 2.179638385772705 secs\n",
            "Rewards Distribution:  [('road (+1)', 16), ('road (-0.1)', 5), ('sand (-1)', 101), ('wall (-5)', 1)]\n",
            "Training took 5.191648721694946 secs\n",
            "Path [[665, 71], [663, 72], [629, 23]]\n",
            "\n",
            "Total Timesteps: 45014 Episode Num: 97 Episode Len: 120 Reward: -88.30000000000001 Time Taken: 2.139577627182007 secs\n",
            "Rewards Distribution:  [('road (+1)', 12), ('road (-0.1)', 13), ('sand (-1)', 94), ('wall (-5)', 1)]\n",
            "Training took 5.054425954818726 secs\n",
            "Path [[865, 100], [866, 100], [821, 30]]\n",
            "\n",
            "Total Timesteps: 45292 Episode Num: 98 Episode Len: 278 Reward: -247.89999999999995 Time Taken: 4.891391277313232 secs\n",
            "Rewards Distribution:  [('road (+1)', 13), ('road (-0.1)', 9), ('sand (-1)', 255), ('wall (-5)', 1)]\n",
            "Training took 11.727499961853027 secs\n",
            "Path [[150, 201], [151, 202], [112, 147], [60, 88]]\n",
            "\n",
            "Total Timesteps: 45870 Episode Num: 99 Episode Len: 578 Reward: -463.80000000000075 Time Taken: 10.117287397384644 secs\n",
            "Rewards Distribution:  [('road (+1)', 33), ('road (-0.1)', 58), ('sand (-1)', 486), ('wall (-5)', 1)]\n",
            "Training took 24.423365354537964 secs\n",
            "Path [[597, 319], [595, 317], [543, 271], [488, 223], [435, 169], [381, 108], [335, 62]]\n",
            "\n",
            "Total Timesteps: 46562 Episode Num: 100 Episode Len: 692 Reward: -442.1000000000015 Time Taken: 12.13405704498291 secs\n",
            "Rewards Distribution:  [('road (+1)', 77), ('road (-0.1)', 111), ('sand (-1)', 503), ('wall (-5)', 1)]\n",
            "Training took 29.2183837890625 secs\n",
            "Path [[1044, 378], [1043, 376], [991, 332], [936, 280], [890, 216], [837, 165], [783, 120], [720, 80]]\n",
            "\n",
            "Total Timesteps: 46567 Episode Num: 101 Episode Len: 5 Reward: -4.3 Time Taken: 0.10686683654785156 secs\n",
            "Rewards Distribution:  [('road (+1)', 1), ('road (-0.1)', 3), ('wall (-5)', 1)]\n",
            "Training took 0.20675373077392578 secs\n",
            "Path [[1095, 29], [1095, 27]]\n",
            "\n",
            "Total Timesteps: 46587 Episode Num: 102 Episode Len: 20 Reward: -6.8 Time Taken: 0.3744521141052246 secs\n",
            "Rewards Distribution:  [('road (+1)', 5), ('road (-0.1)', 8), ('sand (-1)', 6), ('wall (-5)', 1)]\n",
            "Training took 0.8350906372070312 secs\n",
            "Path [[912, 39], [912, 40]]\n",
            "\n",
            "Total Timesteps: 46973 Episode Num: 103 Episode Len: 386 Reward: -326.29999999999984 Time Taken: 6.764024972915649 secs\n",
            "Rewards Distribution:  [('road (+1)', 17), ('road (-0.1)', 33), ('sand (-1)', 335), ('wall (-5)', 1)]\n",
            "Training took 16.303210258483887 secs\n",
            "Path [[578, 220], [576, 218], [524, 171], [470, 123], [417, 57]]\n",
            "\n",
            "Total Timesteps: 47467 Episode Num: 104 Episode Len: 494 Reward: -335.60000000000105 Time Taken: 8.707748174667358 secs\n",
            "Rewards Distribution:  [('road (+1)', 47), ('road (-0.1)', 76), ('sand (-1)', 370), ('wall (-5)', 1)]\n",
            "Training took 20.852150201797485 secs\n",
            "Path [[1375, 255], [1373, 255], [1330, 207], [1279, 161], [1221, 107], [1165, 61]]\n",
            "\n",
            "Total Timesteps: 47916 Episode Num: 105 Episode Len: 449 Reward: -338.09999999999985 Time Taken: 7.895218133926392 secs\n",
            "Rewards Distribution:  [('road (+1)', 39), ('road (-0.1)', 41), ('sand (-1)', 368), ('wall (-5)', 1)]\n",
            "Training took 18.985896110534668 secs\n",
            "Path [[343, 253], [342, 254], [302, 196], [259, 140], [204, 94], [150, 40]]\n",
            "\n",
            "Total Timesteps: 48723 Episode Num: 106 Episode Len: 807 Reward: -642.900000000001 Time Taken: 14.199597597122192 secs\n",
            "Rewards Distribution:  [('road (+1)', 44), ('road (-0.1)', 89), ('sand (-1)', 673), ('wall (-5)', 1)]\n",
            "Training took 34.105222940444946 secs\n",
            "Path [[816, 456], [816, 457], [763, 394], [720, 335], [665, 284], [608, 228], [549, 176], [499, 124], [451, 78], [390, 24]]\n",
            "\n",
            "Total Timesteps: 48871 Episode Num: 107 Episode Len: 148 Reward: -85.7999999999999 Time Taken: 2.6417160034179688 secs\n",
            "Rewards Distribution:  [('road (+1)', 16), ('road (-0.1)', 38), ('sand (-1)', 93), ('wall (-5)', 1)]\n",
            "Training took 6.2691380977630615 secs\n",
            "Path [[1183, 111], [1184, 109], [1114, 61]]\n",
            "\n",
            "Total Timesteps: 49003 Episode Num: 108 Episode Len: 132 Reward: -84.0 Time Taken: 2.3487541675567627 secs\n",
            "Rewards Distribution:  [('road (+1)', 17), ('road (-0.1)', 20), ('sand (-1)', 94), ('wall (-5)', 1)]\n",
            "Training took 5.5759437084198 secs\n",
            "Path [[880, 90], [878, 91], [843, 32]]\n",
            "\n",
            "Total Timesteps: 49181 Episode Num: 109 Episode Len: 178 Reward: -47.60000000000002 Time Taken: 3.153679132461548 secs\n",
            "Rewards Distribution:  [('road (+1)', 42), ('road (-0.1)', 56), ('sand (-1)', 79), ('wall (-5)', 1)]\n",
            "Training took 7.508349895477295 secs\n",
            "Path [[583, 111], [584, 109], [538, 55]]\n",
            "\n",
            "Total Timesteps: 50331 Episode Num: 110 Episode Len: 1150 Reward: -867.8000000000019 Time Taken: 20.421788930892944 secs\n",
            "Rewards Distribution:  [('road (+1)', 81), ('road (-0.1)', 138), ('sand (-1)', 930), ('wall (-5)', 1)]\n",
            "Training took 48.542794704437256 secs\n",
            "Path [[885, 591], [883, 589], [824, 549], [770, 499], [713, 448], [673, 382], [624, 350], [567, 321], [519, 257], [473, 203], [428, 152], [372, 101], [310, 40]]\n",
            "\n",
            "Total Timesteps: 50764 Episode Num: 111 Episode Len: 433 Reward: -377.0000000000005 Time Taken: 7.735021114349365 secs\n",
            "Rewards Distribution:  [('road (+1)', 12), ('road (-0.1)', 40), ('sand (-1)', 380), ('wall (-5)', 1)]\n",
            "Training took 18.305243492126465 secs\n",
            "Path [[1064, 251], [1062, 249], [1012, 205], [957, 158], [904, 104], [849, 33]]\n",
            "\n",
            "Total Timesteps: 50866 Episode Num: 112 Episode Len: 102 Reward: -98.6 Time Taken: 1.8278660774230957 secs\n",
            "Rewards Distribution:  [('road (+1)', 1), ('road (-0.1)', 6), ('sand (-1)', 94), ('wall (-5)', 1)]\n",
            "Training took 4.306439161300659 secs\n",
            "Path [[1014, 73], [1014, 71], [956, 20]]\n",
            "\n",
            "Total Timesteps: 51828 Episode Num: 113 Episode Len: 962 Reward: -831.3000000000014 Time Taken: 16.966832637786865 secs\n",
            "Rewards Distribution:  [('road (+1)', 39), ('road (-0.1)', 63), ('sand (-1)', 859), ('wall (-5)', 1)]\n",
            "Training took 40.638989210128784 secs\n",
            "Path [[593, 500], [591, 500], [543, 453], [492, 407], [438, 357], [388, 302], [331, 252], [283, 206], [229, 151], [176, 97], [131, 44]]\n",
            "\n",
            "Total Timesteps: 52329 Episode Num: 114 Episode Len: 501 Reward: -390.20000000000033 Time Taken: 9.035217761993408 secs\n",
            "Rewards Distribution:  [('road (+1)', 34), ('road (-0.1)', 52), ('sand (-1)', 414), ('wall (-5)', 1)]\n",
            "Training took 21.17641520500183 secs\n",
            "Path [[1330, 281], [1328, 280], [1287, 238], [1232, 192], [1170, 133], [1124, 79], [1080, 20]]\n",
            "\n",
            "Total Timesteps: 53518 Episode Num: 115 Episode Len: 1189 Reward: -983.2000000000013 Time Taken: 21.37700843811035 secs\n",
            "Rewards Distribution:  [('road (+1)', 59), ('road (-0.1)', 102), ('sand (-1)', 1027), ('wall (-5)', 1)]\n",
            "Training took 50.22455883026123 secs\n",
            "Path [[762, 623], [762, 621], [708, 570], [644, 514], [593, 457], [542, 413], [489, 367], [433, 315], [385, 261], [329, 215], [276, 162], [212, 120], [171, 58]]\n",
            "\n",
            "Total Timesteps: 53524 Episode Num: 116 Episode Len: 6 Reward: -7.1 Time Taken: 0.12442135810852051 secs\n",
            "Rewards Distribution:  [('road (+1)', 1), ('road (-0.1)', 1), ('sand (-1)', 3), ('wall (-5)', 1)]\n",
            "Training took 0.24315428733825684 secs\n",
            "Path [[590, 28], [590, 26]]\n",
            "\n",
            "Total Timesteps: 54331 Episode Num: 117 Episode Len: 807 Reward: -667.4000000000005 Time Taken: 14.421075582504272 secs\n",
            "Rewards Distribution:  [('road (+1)', 43), ('road (-0.1)', 64), ('sand (-1)', 699), ('wall (-5)', 1)]\n",
            "Training took 34.08979368209839 secs\n",
            "Path [[671, 455], [672, 454], [611, 392], [556, 345], [513, 286], [463, 240], [409, 191], [362, 128], [317, 76], [269, 30]]\n",
            "\n",
            "Total Timesteps: 55206 Episode Num: 118 Episode Len: 875 Reward: -721.8000000000013 Time Taken: 16.043477535247803 secs\n",
            "Rewards Distribution:  [('road (+1)', 48), ('road (-0.1)', 68), ('sand (-1)', 758), ('wall (-5)', 1)]\n",
            "Training took 36.98886299133301 secs\n",
            "Path [[602, 480], [601, 478], [541, 429], [486, 379], [439, 315], [394, 265], [349, 228], [285, 165], [220, 112], [185, 57]]\n",
            "\n",
            "Total Timesteps: 55532 Episode Num: 119 Episode Len: 326 Reward: -223.6999999999998 Time Taken: 5.979225158691406 secs\n",
            "Rewards Distribution:  [('road (+1)', 32), ('road (-0.1)', 47), ('sand (-1)', 246), ('wall (-5)', 1)]\n",
            "Training took 13.799328088760376 secs\n",
            "Path [[578, 191], [578, 189], [515, 145], [467, 82], [421, 27]]\n",
            "\n",
            "Total Timesteps: 56083 Episode Num: 120 Episode Len: 551 Reward: -412.9999999999999 Time Taken: 9.812105894088745 secs\n",
            "Rewards Distribution:  [('road (+1)', 35), ('road (-0.1)', 80), ('sand (-1)', 435), ('wall (-5)', 1)]\n",
            "Training took 23.291625022888184 secs\n",
            "Path [[717, 306], [717, 304], [653, 261], [603, 197], [555, 149], [509, 103], [450, 50]]\n",
            "\n",
            "Total Timesteps: 56372 Episode Num: 121 Episode Len: 289 Reward: -210.49999999999977 Time Taken: 5.249218702316284 secs\n",
            "Rewards Distribution:  [('road (+1)', 21), ('road (-0.1)', 45), ('sand (-1)', 222), ('wall (-5)', 1)]\n",
            "Training took 12.22245979309082 secs\n",
            "Path [[871, 170], [870, 168], [812, 120], [750, 76]]\n",
            "\n",
            "Total Timesteps: 57118 Episode Num: 122 Episode Len: 746 Reward: -618.300000000001 Time Taken: 13.37985110282898 secs\n",
            "Rewards Distribution:  [('road (+1)', 42), ('road (-0.1)', 53), ('sand (-1)', 650), ('wall (-5)', 1)]\n",
            "Training took 31.528496742248535 secs\n",
            "Path [[386, 583], [387, 584], [343, 521], [297, 471], [248, 425], [185, 381], [142, 321], [88, 274], [45, 225]]\n",
            "\n",
            "Total Timesteps: 57818 Episode Num: 123 Episode Len: 700 Reward: -660.3000000000005 Time Taken: 12.946573972702026 secs\n",
            "Rewards Distribution:  [('road (+1)', 7), ('road (-0.1)', 33), ('sand (-1)', 659), ('wall (-5)', 1)]\n",
            "Training took 29.581722259521484 secs\n",
            "Path [[403, 603], [402, 601], [343, 552], [289, 502], [239, 447], [193, 396], [144, 340], [79, 290]]\n",
            "\n",
            "Total Timesteps: 58247 Episode Num: 124 Episode Len: 429 Reward: -279.4000000000001 Time Taken: 7.7487568855285645 secs\n",
            "Rewards Distribution:  [('road (+1)', 48), ('road (-0.1)', 64), ('sand (-1)', 316), ('wall (-5)', 1)]\n",
            "Training took 18.12732696533203 secs\n",
            "Path [[1098, 233], [1096, 233], [1055, 193], [1004, 148], [950, 98], [901, 43]]\n",
            "\n",
            "Total Timesteps: 58482 Episode Num: 125 Episode Len: 235 Reward: -212.49999999999994 Time Taken: 4.231686353683472 secs\n",
            "Rewards Distribution:  [('road (+1)', 2), ('road (-0.1)', 25), ('sand (-1)', 207), ('wall (-5)', 1)]\n",
            "Training took 9.941076278686523 secs\n",
            "Path [[165, 484], [165, 482], [100, 432], [36, 371]]\n",
            "\n",
            "Total Timesteps: 59263 Episode Num: 126 Episode Len: 781 Reward: -655.0000000000006 Time Taken: 14.163732051849365 secs\n",
            "Rewards Distribution:  [('road (+1)', 38), ('road (-0.1)', 60), ('sand (-1)', 682), ('wall (-5)', 1)]\n",
            "Training took 33.03260517120361 secs\n",
            "Path [[788, 429], [786, 429], [739, 376], [687, 329], [632, 272], [573, 219], [526, 171], [478, 117], [418, 61]]\n",
            "\n",
            "Total Timesteps: 59693 Episode Num: 127 Episode Len: 430 Reward: -309.0999999999999 Time Taken: 7.827540874481201 secs\n",
            "Rewards Distribution:  [('road (+1)', 35), ('road (-0.1)', 61), ('sand (-1)', 333), ('wall (-5)', 1)]\n",
            "Training took 18.17748999595642 secs\n",
            "Path [[1153, 229], [1151, 229], [1108, 182], [1049, 145], [995, 93], [947, 39]]\n",
            "\n",
            "Total Timesteps: 60642 Episode Num: 128 Episode Len: 949 Reward: -781.5000000000015 Time Taken: 17.284050464630127 secs\n",
            "Rewards Distribution:  [('road (+1)', 52), ('road (-0.1)', 75), ('sand (-1)', 821), ('wall (-5)', 1)]\n",
            "Training took 40.115872383117676 secs\n",
            "Path [[591, 509], [592, 510], [543, 443], [497, 391], [450, 345], [395, 300], [334, 252], [289, 197], [244, 140], [191, 94], [137, 42]]\n",
            "\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -531.440000\n",
            "---------------------------------------\n",
            "Total Timesteps: 60719 Episode Num: 129 Episode Len: 77 Reward: -34.10000000000001 Time Taken: 1.3881747722625732 secs\n",
            "Rewards Distribution:  [('road (+1)', 550), ('road (-0.1)', 885), ('sand (-1)', 5755), ('wall (-5)', 11)]\n",
            "Training took 3.279252767562866 secs\n",
            "Path [[531, 75], [530, 73]]\n",
            "\n",
            "Total Timesteps: 61238 Episode Num: 130 Episode Len: 519 Reward: -415.7999999999998 Time Taken: 9.63038182258606 secs\n",
            "Rewards Distribution:  [('road (+1)', 32), ('road (-0.1)', 48), ('sand (-1)', 438), ('wall (-5)', 1)]\n",
            "Training took 21.882261991500854 secs\n",
            "Path [[355, 279], [356, 280], [305, 228], [255, 175], [206, 129], [152, 80], [100, 25]]\n",
            "\n",
            "Total Timesteps: 61629 Episode Num: 131 Episode Len: 391 Reward: -271.3999999999997 Time Taken: 7.048823833465576 secs\n",
            "Rewards Distribution:  [('road (+1)', 33), ('road (-0.1)', 64), ('sand (-1)', 293), ('wall (-5)', 1)]\n",
            "Training took 16.444037914276123 secs\n",
            "Path [[872, 224], [872, 222], [820, 165], [743, 115], [704, 56]]\n",
            "\n",
            "Total Timesteps: 62058 Episode Num: 132 Episode Len: 429 Reward: -361.30000000000035 Time Taken: 7.88412070274353 secs\n",
            "Rewards Distribution:  [('road (+1)', 21), ('road (-0.1)', 33), ('sand (-1)', 374), ('wall (-5)', 1)]\n",
            "Training took 18.03826904296875 secs\n",
            "Path [[1056, 250], [1055, 248], [1004, 205], [950, 155], [900, 100], [857, 29]]\n",
            "\n",
            "Total Timesteps: 62551 Episode Num: 133 Episode Len: 493 Reward: -444.79999999999995 Time Taken: 8.93065619468689 secs\n",
            "Rewards Distribution:  [('road (+1)', 18), ('road (-0.1)', 18), ('sand (-1)', 456), ('wall (-5)', 1)]\n",
            "Training took 20.699448108673096 secs\n",
            "Path [[258, 536], [258, 537], [220, 486], [174, 438], [115, 396], [61, 336]]\n",
            "\n",
            "Total Timesteps: 62825 Episode Num: 134 Episode Len: 274 Reward: -241.29999999999995 Time Taken: 5.041503667831421 secs\n",
            "Rewards Distribution:  [('road (+1)', 8), ('road (-0.1)', 23), ('sand (-1)', 242), ('wall (-5)', 1)]\n",
            "Training took 11.550696849822998 secs\n",
            "Path [[161, 456], [160, 457], [107, 410], [56, 355]]\n",
            "\n",
            "Total Timesteps: 63693 Episode Num: 135 Episode Len: 868 Reward: -610.3000000000025 Time Taken: 15.9635329246521 secs\n",
            "Rewards Distribution:  [('road (+1)', 80), ('road (-0.1)', 113), ('sand (-1)', 674), ('wall (-5)', 1)]\n",
            "Training took 36.535932779312134 secs\n",
            "Path [[1091, 462], [1092, 460], [1037, 405], [988, 341], [942, 288], [896, 243], [832, 199], [770, 141], [731, 97], [680, 49]]\n",
            "\n",
            "Total Timesteps: 64045 Episode Num: 136 Episode Len: 352 Reward: -189.29999999999995 Time Taken: 6.677653074264526 secs\n",
            "Rewards Distribution:  [('road (+1)', 46), ('road (-0.1)', 83), ('sand (-1)', 222), ('wall (-5)', 1)]\n",
            "Training took 14.906758308410645 secs\n",
            "Path [[877, 206], [876, 207], [831, 159], [781, 106], [722, 51]]\n",
            "\n",
            "Total Timesteps: 65262 Episode Num: 137 Episode Len: 1217 Reward: -913.4000000000018 Time Taken: 23.40953278541565 secs\n",
            "Rewards Distribution:  [('road (+1)', 89), ('road (-0.1)', 144), ('sand (-1)', 983), ('wall (-5)', 1)]\n",
            "Training took 51.49234700202942 secs\n",
            "Path [[780, 623], [778, 621], [731, 579], [664, 526], [612, 472], [557, 424], [510, 378], [450, 342], [399, 278], [350, 224], [300, 174], [247, 127], [192, 74], [143, 20]]\n",
            "\n",
            "Total Timesteps: 65762 Episode Num: 138 Episode Len: 500 Reward: -434.40000000000043 Time Taken: 9.155047416687012 secs\n",
            "Rewards Distribution:  [('road (+1)', 24), ('road (-0.1)', 24), ('sand (-1)', 451), ('wall (-5)', 1)]\n",
            "Training took 21.155646324157715 secs\n",
            "Path [[269, 377], [269, 378], [229, 318], [183, 271], [131, 225], [69, 165]]\n",
            "\n",
            "Total Timesteps: 66046 Episode Num: 139 Episode Len: 284 Reward: -206.39999999999975 Time Taken: 5.167468547821045 secs\n",
            "Rewards Distribution:  [('road (+1)', 21), ('road (-0.1)', 44), ('sand (-1)', 218), ('wall (-5)', 1)]\n",
            "Training took 11.997669458389282 secs\n",
            "Path [[356, 169], [356, 167], [302, 118], [247, 63]]\n",
            "\n",
            "Total Timesteps: 66291 Episode Num: 140 Episode Len: 245 Reward: -182.99999999999997 Time Taken: 4.492665529251099 secs\n",
            "Rewards Distribution:  [('road (+1)', 24), ('road (-0.1)', 20), ('sand (-1)', 200), ('wall (-5)', 1)]\n",
            "Training took 10.370988845825195 secs\n",
            "Path [[131, 352], [129, 353], [87, 298], [40, 252]]\n",
            "\n",
            "Total Timesteps: 66352 Episode Num: 141 Episode Len: 61 Reward: -32.60000000000001 Time Taken: 1.1258950233459473 secs\n",
            "Rewards Distribution:  [('road (+1)', 9), ('road (-0.1)', 16), ('sand (-1)', 35), ('wall (-5)', 1)]\n",
            "Training took 2.5753538608551025 secs\n",
            "Path [[1067, 67], [1066, 68]]\n",
            "\n",
            "Total Timesteps: 66357 Episode Num: 142 Episode Len: 5 Reward: -5.4 Time Taken: 0.10831141471862793 secs\n",
            "Rewards Distribution:  [('road (-0.1)', 4), ('wall (-5)', 1)]\n",
            "Training took 0.20740652084350586 secs\n",
            "Path [[1033, 29], [1032, 27]]\n",
            "\n",
            "Total Timesteps: 66844 Episode Num: 143 Episode Len: 487 Reward: -388.6 Time Taken: 8.918541193008423 secs\n",
            "Rewards Distribution:  [('road (+1)', 26), ('road (-0.1)', 56), ('sand (-1)', 404), ('wall (-5)', 1)]\n",
            "Training took 20.61244511604309 secs\n",
            "Path [[645, 255], [643, 256], [597, 208], [546, 167], [492, 119], [434, 58]]\n",
            "\n",
            "Total Timesteps: 66857 Episode Num: 144 Episode Len: 13 Reward: 7 Time Taken: 0.2694661617279053 secs\n",
            "Rewards Distribution:  [('road (+1)', 12), ('wall (-5)', 1)]\n",
            "Training took 0.5472919940948486 secs\n",
            "Path [[1144, 24], [1144, 25]]\n",
            "\n",
            "Total Timesteps: 67311 Episode Num: 145 Episode Len: 454 Reward: -381.1000000000004 Time Taken: 8.380831956863403 secs\n",
            "Rewards Distribution:  [('road (+1)', 20), ('road (-0.1)', 41), ('sand (-1)', 392), ('wall (-5)', 1)]\n",
            "Training took 19.192176580429077 secs\n",
            "Path [[1095, 260], [1093, 261], [1048, 209], [1000, 163], [946, 115], [893, 51]]\n",
            "\n",
            "Total Timesteps: 67764 Episode Num: 146 Episode Len: 453 Reward: -320.79999999999984 Time Taken: 8.364727258682251 secs\n",
            "Rewards Distribution:  [('road (+1)', 42), ('road (-0.1)', 58), ('sand (-1)', 352), ('wall (-5)', 1)]\n",
            "Training took 19.162211656570435 secs\n",
            "Path [[253, 594], [254, 592], [197, 526], [142, 480], [96, 429], [51, 375]]\n",
            "\n",
            "Total Timesteps: 68263 Episode Num: 147 Episode Len: 499 Reward: -405.0000000000004 Time Taken: 9.03332805633545 secs\n",
            "Rewards Distribution:  [('road (+1)', 31), ('road (-0.1)', 40), ('sand (-1)', 427), ('wall (-5)', 1)]\n",
            "Training took 21.110044717788696 secs\n",
            "Path [[1322, 288], [1320, 288], [1280, 243], [1227, 198], [1171, 134], [1123, 79]]\n",
            "\n",
            "Total Timesteps: 68763 Episode Num: 148 Episode Len: 500 Reward: -358.9999999999998 Time Taken: 9.194331407546997 secs\n",
            "Rewards Distribution:  [('road (+1)', 41), ('road (-0.1)', 70), ('sand (-1)', 388), ('wall (-5)', 1)]\n",
            "Training took 21.14240264892578 secs\n",
            "Path [[281, 566], [279, 565], [222, 511], [168, 465], [111, 412], [64, 358]]\n",
            "\n",
            "Total Timesteps: 69180 Episode Num: 149 Episode Len: 417 Reward: -310.6999999999997 Time Taken: 7.791767120361328 secs\n",
            "Rewards Distribution:  [('road (+1)', 34), ('road (-0.1)', 47), ('sand (-1)', 335), ('wall (-5)', 1)]\n",
            "Training took 17.613559007644653 secs\n",
            "Path [[231, 382], [232, 380], [180, 322], [127, 270], [78, 215], [33, 175]]\n",
            "\n",
            "Total Timesteps: 69590 Episode Num: 150 Episode Len: 410 Reward: -336.7000000000001 Time Taken: 7.456372976303101 secs\n",
            "Rewards Distribution:  [('road (+1)', 22), ('road (-0.1)', 37), ('sand (-1)', 350), ('wall (-5)', 1)]\n",
            "Training took 17.328355073928833 secs\n",
            "Path [[1067, 240], [1066, 241], [1027, 181], [981, 134], [921, 94], [868, 22]]\n",
            "\n",
            "Total Timesteps: 69704 Episode Num: 151 Episode Len: 114 Reward: -89.4 Time Taken: 2.1387710571289062 secs\n",
            "Rewards Distribution:  [('road (+1)', 8), ('road (-0.1)', 14), ('sand (-1)', 91), ('wall (-5)', 1)]\n",
            "Training took 4.815303325653076 secs\n",
            "Path [[887, 100], [888, 99], [838, 26]]\n",
            "\n",
            "Total Timesteps: 69783 Episode Num: 152 Episode Len: 79 Reward: -49.800000000000026 Time Taken: 1.4573233127593994 secs\n",
            "Rewards Distribution:  [('road (+1)', 4), ('road (-0.1)', 28), ('sand (-1)', 46), ('wall (-5)', 1)]\n",
            "Training took 3.3341598510742188 secs\n",
            "Path [[1048, 77], [1049, 75]]\n",
            "\n",
            "Total Timesteps: 70495 Episode Num: 153 Episode Len: 712 Reward: -552.100000000001 Time Taken: 12.727303504943848 secs\n",
            "Rewards Distribution:  [('road (+1)', 50), ('road (-0.1)', 71), ('sand (-1)', 590), ('wall (-5)', 1)]\n",
            "Training took 30.0450177192688 secs\n",
            "Path [[751, 391], [750, 392], [709, 332], [656, 282], [602, 236], [546, 187], [499, 122], [454, 75], [401, 30]]\n",
            "\n",
            "Total Timesteps: 70976 Episode Num: 154 Episode Len: 481 Reward: -307.89999999999975 Time Taken: 8.576250791549683 secs\n",
            "Rewards Distribution:  [('road (+1)', 53), ('road (-0.1)', 79), ('sand (-1)', 348), ('wall (-5)', 1)]\n",
            "Training took 20.322084188461304 secs\n",
            "Path [[272, 610], [273, 608], [210, 549], [159, 489], [113, 441], [63, 398]]\n",
            "\n",
            "Total Timesteps: 71198 Episode Num: 155 Episode Len: 222 Reward: -185.9 Time Taken: 3.995291233062744 secs\n",
            "Rewards Distribution:  [('road (+1)', 16), ('road (-0.1)', 9), ('sand (-1)', 196), ('wall (-5)', 1)]\n",
            "Training took 9.371533870697021 secs\n",
            "Path [[124, 552], [122, 553], [86, 506], [36, 460]]\n",
            "\n",
            "Total Timesteps: 71693 Episode Num: 156 Episode Len: 495 Reward: -332.39999999999964 Time Taken: 8.781279802322388 secs\n",
            "Rewards Distribution:  [('road (+1)', 50), ('road (-0.1)', 74), ('sand (-1)', 370), ('wall (-5)', 1)]\n",
            "Training took 20.90928077697754 secs\n",
            "Path [[941, 259], [940, 260], [906, 209], [859, 170], [809, 129], [742, 78]]\n",
            "\n",
            "Total Timesteps: 72504 Episode Num: 157 Episode Len: 811 Reward: -587.400000000001 Time Taken: 14.440767526626587 secs\n",
            "Rewards Distribution:  [('road (+1)', 67), ('road (-0.1)', 104), ('sand (-1)', 639), ('wall (-5)', 1)]\n",
            "Training took 34.25931787490845 secs\n",
            "Path [[808, 443], [809, 442], [748, 393], [705, 330], [654, 276], [604, 231], [546, 189], [490, 126], [441, 72], [398, 29]]\n",
            "\n",
            "Total Timesteps: 73012 Episode Num: 158 Episode Len: 508 Reward: -315.20000000000056 Time Taken: 9.133455753326416 secs\n",
            "Rewards Distribution:  [('road (+1)', 48), ('road (-0.1)', 112), ('sand (-1)', 347), ('wall (-5)', 1)]\n",
            "Training took 21.47322177886963 secs\n",
            "Path [[846, 292], [844, 290], [795, 247], [739, 201], [688, 147], [643, 95], [577, 32]]\n",
            "\n",
            "Total Timesteps: 73803 Episode Num: 159 Episode Len: 791 Reward: -579.9000000000015 Time Taken: 14.11403751373291 secs\n",
            "Rewards Distribution:  [('road (+1)', 63), ('road (-0.1)', 99), ('sand (-1)', 628), ('wall (-5)', 1)]\n",
            "Training took 33.41621804237366 secs\n",
            "Path [[598, 409], [596, 408], [543, 365], [483, 325], [435, 264], [389, 210], [343, 163], [291, 117], [231, 74]]\n",
            "\n",
            "Total Timesteps: 75019 Episode Num: 160 Episode Len: 1216 Reward: -937.2000000000011 Time Taken: 21.73348903656006 secs\n",
            "Rewards Distribution:  [('road (+1)', 82), ('road (-0.1)', 132), ('sand (-1)', 1001), ('wall (-5)', 1)]\n",
            "Training took 51.37350082397461 secs\n",
            "Path [[793, 626], [794, 627], [754, 563], [704, 514], [651, 479], [598, 441], [541, 385], [496, 333], [447, 282], [393, 236], [329, 188], [287, 126], [225, 79], [173, 34]]\n",
            "\n",
            "Total Timesteps: 76072 Episode Num: 161 Episode Len: 1053 Reward: -817.9000000000017 Time Taken: 18.85655975341797 secs\n",
            "Rewards Distribution:  [('road (+1)', 75), ('road (-0.1)', 99), ('sand (-1)', 878), ('wall (-5)', 1)]\n",
            "Training took 44.473554611206055 secs\n",
            "Path [[934, 570], [934, 568], [880, 518], [826, 467], [778, 409], [729, 357], [671, 315], [618, 254], [567, 200], [521, 150], [468, 99], [415, 49]]\n",
            "\n",
            "Total Timesteps: 76346 Episode Num: 162 Episode Len: 274 Reward: -155.39999999999995 Time Taken: 4.947163105010986 secs\n",
            "Rewards Distribution:  [('road (+1)', 37), ('road (-0.1)', 54), ('sand (-1)', 182), ('wall (-5)', 1)]\n",
            "Training took 11.584327220916748 secs\n",
            "Path [[825, 169], [825, 170], [782, 117], [736, 56]]\n",
            "\n",
            "Total Timesteps: 76878 Episode Num: 163 Episode Len: 532 Reward: -469.90000000000066 Time Taken: 9.382977962493896 secs\n",
            "Rewards Distribution:  [('road (+1)', 20), ('road (-0.1)', 29), ('sand (-1)', 482), ('wall (-5)', 1)]\n",
            "Training took 22.477738618850708 secs\n",
            "Path [[571, 293], [569, 293], [523, 245], [472, 199], [415, 138], [354, 80], [320, 33]]\n",
            "\n",
            "Total Timesteps: 77651 Episode Num: 164 Episode Len: 773 Reward: -591.7000000000014 Time Taken: 13.991456031799316 secs\n",
            "Rewards Distribution:  [('road (+1)', 49), ('road (-0.1)', 97), ('sand (-1)', 626), ('wall (-5)', 1)]\n",
            "Training took 32.69281077384949 secs\n",
            "Path [[601, 414], [602, 413], [537, 360], [483, 304], [437, 253], [388, 207], [333, 158], [270, 106], [233, 51]]\n",
            "\n",
            "Total Timesteps: 78352 Episode Num: 165 Episode Len: 701 Reward: -608.600000000001 Time Taken: 12.842261552810669 secs\n",
            "Rewards Distribution:  [('road (+1)', 23), ('road (-0.1)', 56), ('sand (-1)', 621), ('wall (-5)', 1)]\n",
            "Training took 29.67082691192627 secs\n",
            "Path [[396, 543], [397, 541], [334, 481], [283, 427], [234, 365], [185, 319], [136, 281], [61, 230], [20, 170]]\n",
            "\n",
            "Total Timesteps: 79378 Episode Num: 166 Episode Len: 1026 Reward: -772.8000000000022 Time Taken: 18.559887409210205 secs\n",
            "Rewards Distribution:  [('road (+1)', 80), ('road (-0.1)', 108), ('sand (-1)', 837), ('wall (-5)', 1)]\n",
            "Training took 43.41558289527893 secs\n",
            "Path [[949, 556], [949, 557], [913, 501], [867, 453], [817, 402], [756, 342], [707, 286], [657, 236], [608, 191], [546, 144], [493, 79], [448, 26]]\n",
            "\n",
            "Total Timesteps: 79643 Episode Num: 167 Episode Len: 265 Reward: -201.8 Time Taken: 4.850840330123901 secs\n",
            "Rewards Distribution:  [('road (+1)', 21), ('road (-0.1)', 28), ('sand (-1)', 215), ('wall (-5)', 1)]\n",
            "Training took 11.243203401565552 secs\n",
            "Path [[151, 392], [152, 391], [97, 330], [47, 275]]\n",
            "\n",
            "Total Timesteps: 79928 Episode Num: 168 Episode Len: 285 Reward: -199.19999999999976 Time Taken: 5.206416130065918 secs\n",
            "Rewards Distribution:  [('road (+1)', 26), ('road (-0.1)', 42), ('sand (-1)', 216), ('wall (-5)', 1)]\n",
            "Training took 12.06210970878601 secs\n",
            "Path [[376, 160], [374, 158], [324, 111], [266, 63]]\n",
            "\n",
            "Total Timesteps: 79941 Episode Num: 169 Episode Len: 13 Reward: -10.5 Time Taken: 0.2604186534881592 secs\n",
            "Rewards Distribution:  [('road (+1)', 1), ('road (-0.1)', 5), ('sand (-1)', 6), ('wall (-5)', 1)]\n",
            "Training took 0.553692102432251 secs\n",
            "Path [[874, 39], [875, 37]]\n",
            "\n",
            "Total Timesteps: 80033 Episode Num: 170 Episode Len: 92 Reward: -58.50000000000001 Time Taken: 1.7137813568115234 secs\n",
            "Rewards Distribution:  [('road (+1)', 12), ('road (-0.1)', 15), ('sand (-1)', 64), ('wall (-5)', 1)]\n",
            "Training took 3.8952932357788086 secs\n",
            "Path [[949, 87], [950, 88]]\n",
            "\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -377.320000\n",
            "---------------------------------------\n",
            "Total Timesteps: 80128 Episode Num: 171 Episode Len: 95 Reward: -88.3 Time Taken: 1.754343032836914 secs\n",
            "Rewards Distribution:  [('road (+1)', 376), ('road (-0.1)', 565), ('sand (-1)', 4126), ('wall (-5)', 11)]\n",
            "Training took 4.03852915763855 secs\n",
            "Path [[71, 569], [72, 567]]\n",
            "\n",
            "Total Timesteps: 80146 Episode Num: 172 Episode Len: 18 Reward: 9.8 Time Taken: 0.3614518642425537 secs\n",
            "Rewards Distribution:  [('road (+1)', 15), ('road (-0.1)', 2), ('wall (-5)', 1)]\n",
            "Training took 0.7497694492340088 secs\n",
            "Path [[522, 27], [522, 28]]\n",
            "\n",
            "Total Timesteps: 80150 Episode Num: 173 Episode Len: 4 Reward: -2 Time Taken: 0.09275937080383301 secs\n",
            "Rewards Distribution:  [('road (+1)', 3), ('wall (-5)', 1)]\n",
            "Training took 0.16204428672790527 secs\n",
            "Path [[1378, 24], [1379, 23]]\n",
            "\n",
            "Total Timesteps: 80276 Episode Num: 174 Episode Len: 126 Reward: -103.9 Time Taken: 2.4468159675598145 secs\n",
            "Rewards Distribution:  [('road (+1)', 9), ('road (-0.1)', 9), ('sand (-1)', 107), ('wall (-5)', 1)]\n",
            "Training took 5.30218243598938 secs\n",
            "Path [[688, 82], [687, 83], [647, 25]]\n",
            "\n",
            "Total Timesteps: 80638 Episode Num: 175 Episode Len: 362 Reward: -302.1000000000001 Time Taken: 6.703601121902466 secs\n",
            "Rewards Distribution:  [('road (+1)', 18), ('road (-0.1)', 31), ('sand (-1)', 312), ('wall (-5)', 1)]\n",
            "Training took 15.269947528839111 secs\n",
            "Path [[1069, 211], [1067, 210], [1019, 164], [966, 119], [912, 66]]\n",
            "\n",
            "Total Timesteps: 80673 Episode Num: 176 Episode Len: 35 Reward: 10.3 Time Taken: 0.6631193161010742 secs\n",
            "Rewards Distribution:  [('road (+1)', 17), ('road (-0.1)', 17), ('wall (-5)', 1)]\n",
            "Training took 1.4679710865020752 secs\n",
            "Path [[1085, 32], [1084, 30]]\n",
            "\n",
            "Total Timesteps: 81581 Episode Num: 177 Episode Len: 908 Reward: -765.6000000000012 Time Taken: 16.62070369720459 secs\n",
            "Rewards Distribution:  [('road (+1)', 39), ('road (-0.1)', 76), ('sand (-1)', 792), ('wall (-5)', 1)]\n",
            "Training took 38.21822929382324 secs\n",
            "Path [[605, 479], [606, 479], [543, 423], [498, 369], [452, 326], [398, 277], [333, 227], [285, 166], [239, 119], [186, 72], [132, 21]]\n",
            "\n",
            "Total Timesteps: 81732 Episode Num: 178 Episode Len: 151 Reward: -153.2 Time Taken: 2.82194185256958 secs\n",
            "Rewards Distribution:  [('road (-0.1)', 2), ('sand (-1)', 148), ('wall (-5)', 1)]\n",
            "Training took 6.366266965866089 secs\n",
            "Path [[95, 126], [95, 124], [41, 75]]\n",
            "\n",
            "Total Timesteps: 81882 Episode Num: 179 Episode Len: 150 Reward: -112.1 Time Taken: 2.8077597618103027 secs\n",
            "Rewards Distribution:  [('road (+1)', 16), ('road (-0.1)', 11), ('sand (-1)', 122), ('wall (-5)', 1)]\n",
            "Training took 6.31307315826416 secs\n",
            "Path [[89, 558], [87, 558], [50, 514]]\n",
            "\n",
            "Total Timesteps: 82583 Episode Num: 180 Episode Len: 701 Reward: -590.6000000000005 Time Taken: 13.110836505889893 secs\n",
            "Rewards Distribution:  [('road (+1)', 32), ('road (-0.1)', 56), ('sand (-1)', 612), ('wall (-5)', 1)]\n",
            "Training took 29.4926176071167 secs\n",
            "Path [[890, 400], [891, 400], [843, 343], [795, 279], [740, 231], [684, 182], [618, 132], [582, 74], [535, 20]]\n",
            "\n",
            "Total Timesteps: 82983 Episode Num: 181 Episode Len: 400 Reward: -357.50000000000057 Time Taken: 7.226787805557251 secs\n",
            "Rewards Distribution:  [('road (+1)', 12), ('road (-0.1)', 25), ('sand (-1)', 362), ('wall (-5)', 1)]\n",
            "Training took 16.91386318206787 secs\n",
            "Path [[1057, 239], [1058, 238], [1009, 182], [961, 128], [913, 76]]\n",
            "\n",
            "Total Timesteps: 84096 Episode Num: 182 Episode Len: 1113 Reward: -810.600000000001 Time Taken: 20.175515174865723 secs\n",
            "Rewards Distribution:  [('road (+1)', 92), ('road (-0.1)', 136), ('sand (-1)', 884), ('wall (-5)', 1)]\n",
            "Training took 47.08075428009033 secs\n",
            "Path [[860, 587], [858, 587], [806, 543], [754, 498], [695, 450], [643, 387], [598, 337], [548, 290], [494, 242], [442, 188], [392, 126], [345, 82], [291, 34]]\n",
            "\n",
            "Total Timesteps: 84575 Episode Num: 183 Episode Len: 479 Reward: -327.2999999999996 Time Taken: 8.81411600112915 secs\n",
            "Rewards Distribution:  [('road (+1)', 45), ('road (-0.1)', 73), ('sand (-1)', 360), ('wall (-5)', 1)]\n",
            "Training took 20.264930725097656 secs\n",
            "Path [[945, 269], [946, 267], [893, 212], [842, 156], [801, 112], [746, 67]]\n",
            "\n",
            "Total Timesteps: 85712 Episode Num: 184 Episode Len: 1137 Reward: -885.1000000000016 Time Taken: 20.39283514022827 secs\n",
            "Rewards Distribution:  [('road (+1)', 69), ('road (-0.1)', 131), ('sand (-1)', 936), ('wall (-5)', 1)]\n",
            "Training took 48.07709050178528 secs\n",
            "Path [[828, 600], [829, 600], [781, 534], [735, 480], [681, 450], [616, 405], [551, 350], [504, 295], [458, 247], [406, 202], [352, 151], [292, 91], [256, 45]]\n",
            "\n",
            "Total Timesteps: 86407 Episode Num: 185 Episode Len: 695 Reward: -505.6000000000003 Time Taken: 12.369874000549316 secs\n",
            "Rewards Distribution:  [('road (+1)', 58), ('road (-0.1)', 86), ('sand (-1)', 550), ('wall (-5)', 1)]\n",
            "Training took 29.38396430015564 secs\n",
            "Path [[1207, 372], [1208, 373], [1168, 311], [1117, 271], [1076, 237], [1018, 180], [966, 125], [916, 69]]\n",
            "\n",
            "Total Timesteps: 87034 Episode Num: 186 Episode Len: 627 Reward: -533.8000000000002 Time Taken: 11.131354570388794 secs\n",
            "Rewards Distribution:  [('road (+1)', 27), ('road (-0.1)', 48), ('sand (-1)', 551), ('wall (-5)', 1)]\n",
            "Training took 26.53225088119507 secs\n",
            "Path [[404, 345], [405, 345], [358, 288], [311, 240], [265, 192], [204, 141], [146, 83], [99, 29]]\n",
            "\n",
            "Total Timesteps: 87494 Episode Num: 187 Episode Len: 460 Reward: -346.5 Time Taken: 8.380687475204468 secs\n",
            "Rewards Distribution:  [('road (+1)', 34), ('road (-0.1)', 55), ('sand (-1)', 370), ('wall (-5)', 1)]\n",
            "Training took 19.44239592552185 secs\n",
            "Path [[1344, 257], [1342, 257], [1297, 209], [1245, 163], [1187, 121], [1136, 62]]\n",
            "\n",
            "Total Timesteps: 87537 Episode Num: 188 Episode Len: 43 Reward: -28.5 Time Taken: 0.8004302978515625 secs\n",
            "Rewards Distribution:  [('road (+1)', 7), ('road (-0.1)', 5), ('sand (-1)', 30), ('wall (-5)', 1)]\n",
            "Training took 1.8139958381652832 secs\n",
            "Path [[847, 50], [848, 50]]\n",
            "\n",
            "Total Timesteps: 88142 Episode Num: 189 Episode Len: 605 Reward: -490.60000000000105 Time Taken: 10.971150875091553 secs\n",
            "Rewards Distribution:  [('road (+1)', 34), ('road (-0.1)', 56), ('sand (-1)', 514), ('wall (-5)', 1)]\n",
            "Training took 25.568648099899292 secs\n",
            "Path [[1273, 329], [1271, 330], [1230, 275], [1182, 229], [1128, 182], [1075, 127], [1022, 73], [974, 25]]\n",
            "\n",
            "Total Timesteps: 88302 Episode Num: 190 Episode Len: 160 Reward: -160.2 Time Taken: 2.9636785984039307 secs\n",
            "Rewards Distribution:  [('road (+1)', 1), ('road (-0.1)', 2), ('sand (-1)', 156), ('wall (-5)', 1)]\n",
            "Training took 6.753073215484619 secs\n",
            "Path [[99, 127], [100, 125], [47, 73]]\n",
            "\n",
            "Total Timesteps: 88646 Episode Num: 191 Episode Len: 344 Reward: -291.7999999999998 Time Taken: 6.139033317565918 secs\n",
            "Rewards Distribution:  [('road (+1)', 11), ('road (-0.1)', 38), ('sand (-1)', 294), ('wall (-5)', 1)]\n",
            "Training took 14.55877423286438 secs\n",
            "Path [[207, 366], [208, 364], [154, 314], [94, 263], [44, 212]]\n",
            "\n",
            "Total Timesteps: 88902 Episode Num: 192 Episode Len: 256 Reward: -198.30000000000004 Time Taken: 4.633509874343872 secs\n",
            "Rewards Distribution:  [('road (+1)', 16), ('road (-0.1)', 33), ('sand (-1)', 206), ('wall (-5)', 1)]\n",
            "Training took 10.808677434921265 secs\n",
            "Path [[158, 445], [158, 443], [89, 400], [39, 338]]\n",
            "\n",
            "Total Timesteps: 89173 Episode Num: 193 Episode Len: 271 Reward: -248.49999999999997 Time Taken: 4.803480625152588 secs\n",
            "Rewards Distribution:  [('road (+1)', 11), ('road (-0.1)', 5), ('sand (-1)', 254), ('wall (-5)', 1)]\n",
            "Training took 11.4461190700531 secs\n",
            "Path [[146, 199], [147, 200], [101, 146], [53, 83]]\n",
            "\n",
            "Total Timesteps: 89946 Episode Num: 194 Episode Len: 773 Reward: -538.0000000000011 Time Taken: 13.795161247253418 secs\n",
            "Rewards Distribution:  [('road (+1)', 70), ('road (-0.1)', 110), ('sand (-1)', 592), ('wall (-5)', 1)]\n",
            "Training took 32.66228675842285 secs\n",
            "Path [[821, 436], [822, 437], [761, 368], [715, 315], [665, 262], [599, 216], [544, 160], [496, 100], [450, 54]]\n",
            "\n",
            "Total Timesteps: 90201 Episode Num: 195 Episode Len: 255 Reward: -212.49999999999986 Time Taken: 4.490087032318115 secs\n",
            "Rewards Distribution:  [('road (+1)', 12), ('road (-0.1)', 25), ('sand (-1)', 217), ('wall (-5)', 1)]\n",
            "Training took 10.805553197860718 secs\n",
            "Path [[870, 168], [871, 166], [818, 112], [764, 43]]\n",
            "\n",
            "Total Timesteps: 90218 Episode Num: 196 Episode Len: 17 Reward: -0.4999999999999982 Time Taken: 0.322037935256958 secs\n",
            "Rewards Distribution:  [('road (+1)', 8), ('road (-0.1)', 5), ('sand (-1)', 3), ('wall (-5)', 1)]\n",
            "Training took 0.7159872055053711 secs\n",
            "Path [[883, 43], [884, 43]]\n",
            "\n",
            "Total Timesteps: 90418 Episode Num: 197 Episode Len: 200 Reward: -94.39999999999988 Time Taken: 3.5831120014190674 secs\n",
            "Rewards Distribution:  [('road (+1)', 35), ('road (-0.1)', 44), ('sand (-1)', 120), ('wall (-5)', 1)]\n",
            "Training took 8.444334983825684 secs\n",
            "Path [[120, 237], [121, 235], [69, 178]]\n",
            "\n",
            "Total Timesteps: 90597 Episode Num: 198 Episode Len: 179 Reward: -173.8 Time Taken: 3.1499717235565186 secs\n",
            "Rewards Distribution:  [('road (+1)', 1), ('road (-0.1)', 8), ('sand (-1)', 169), ('wall (-5)', 1)]\n",
            "Training took 7.557512521743774 secs\n",
            "Path [[246, 115], [246, 113], [186, 61]]\n",
            "\n",
            "Total Timesteps: 90615 Episode Num: 199 Episode Len: 18 Reward: 9.8 Time Taken: 0.3426690101623535 secs\n",
            "Rewards Distribution:  [('road (+1)', 15), ('road (-0.1)', 2), ('wall (-5)', 1)]\n",
            "Training took 0.7490665912628174 secs\n",
            "Path [[1304, 27], [1304, 28]]\n",
            "\n",
            "Total Timesteps: 91347 Episode Num: 200 Episode Len: 732 Reward: -581.2000000000003 Time Taken: 12.972939014434814 secs\n",
            "Rewards Distribution:  [('road (+1)', 45), ('road (-0.1)', 72), ('sand (-1)', 614), ('wall (-5)', 1)]\n",
            "Training took 30.9080593585968 secs\n",
            "Path [[898, 399], [899, 397], [847, 344], [791, 281], [752, 243], [701, 193], [647, 146], [588, 83], [541, 29]]\n",
            "\n",
            "Total Timesteps: 91440 Episode Num: 201 Episode Len: 93 Reward: -29.10000000000003 Time Taken: 1.6632423400878906 secs\n",
            "Rewards Distribution:  [('road (+1)', 20), ('road (-0.1)', 31), ('sand (-1)', 41), ('wall (-5)', 1)]\n",
            "Training took 3.9305927753448486 secs\n",
            "Path [[1164, 65], [1162, 66]]\n",
            "\n",
            "Total Timesteps: 91879 Episode Num: 202 Episode Len: 439 Reward: -336.60000000000025 Time Taken: 7.813727617263794 secs\n",
            "Rewards Distribution:  [('road (+1)', 28), ('road (-0.1)', 56), ('sand (-1)', 354), ('wall (-5)', 1)]\n",
            "Training took 18.57169461250305 secs\n",
            "Path [[660, 249], [661, 248], [609, 190], [543, 136], [500, 84], [451, 39]]\n",
            "\n",
            "Total Timesteps: 91907 Episode Num: 203 Episode Len: 28 Reward: -24.8 Time Taken: 0.5123405456542969 secs\n",
            "Rewards Distribution:  [('road (-0.1)', 8), ('sand (-1)', 19), ('wall (-5)', 1)]\n",
            "Training took 1.1783456802368164 secs\n",
            "Path [[229, 25], [227, 24]]\n",
            "\n",
            "Total Timesteps: 92017 Episode Num: 204 Episode Len: 110 Reward: -93.19999999999995 Time Taken: 1.9871256351470947 secs\n",
            "Rewards Distribution:  [('road (+1)', 5), ('road (-0.1)', 12), ('sand (-1)', 92), ('wall (-5)', 1)]\n",
            "Training took 4.634741306304932 secs\n",
            "Path [[561, 88], [562, 87], [506, 22]]\n",
            "\n",
            "Total Timesteps: 92383 Episode Num: 205 Episode Len: 366 Reward: -257.0 Time Taken: 6.543705940246582 secs\n",
            "Rewards Distribution:  [('road (+1)', 34), ('road (-0.1)', 50), ('sand (-1)', 281), ('wall (-5)', 1)]\n",
            "Training took 15.45052695274353 secs\n",
            "Path [[210, 545], [208, 543], [167, 509], [104, 451], [47, 388]]\n",
            "\n",
            "Total Timesteps: 92862 Episode Num: 206 Episode Len: 479 Reward: -347.3000000000016 Time Taken: 8.404781103134155 secs\n",
            "Rewards Distribution:  [('road (+1)', 26), ('road (-0.1)', 93), ('sand (-1)', 359), ('wall (-5)', 1)]\n",
            "Training took 20.25674843788147 secs\n",
            "Path [[847, 288], [847, 286], [792, 232], [729, 183], [678, 125], [639, 78]]\n",
            "\n",
            "Total Timesteps: 92888 Episode Num: 207 Episode Len: 26 Reward: 5.6 Time Taken: 0.5156068801879883 secs\n",
            "Rewards Distribution:  [('road (+1)', 16), ('road (-0.1)', 4), ('sand (-1)', 5), ('wall (-5)', 1)]\n",
            "Training took 1.0949764251708984 secs\n",
            "Path [[875, 27], [873, 28]]\n",
            "\n",
            "Total Timesteps: 93443 Episode Num: 208 Episode Len: 555 Reward: -469.0000000000007 Time Taken: 9.956925630569458 secs\n",
            "Rewards Distribution:  [('road (+1)', 27), ('road (-0.1)', 40), ('sand (-1)', 487), ('wall (-5)', 1)]\n",
            "Training took 23.46873140335083 secs\n",
            "Path [[578, 326], [579, 326], [524, 259], [478, 205], [432, 155], [374, 104], [319, 50]]\n",
            "\n",
            "Total Timesteps: 93636 Episode Num: 209 Episode Len: 193 Reward: -153.10000000000002 Time Taken: 3.4503872394561768 secs\n",
            "Rewards Distribution:  [('road (+1)', 17), ('road (-0.1)', 11), ('sand (-1)', 164), ('wall (-5)', 1)]\n",
            "Training took 8.167435884475708 secs\n",
            "Path [[684, 130], [685, 131], [650, 61]]\n",
            "\n",
            "Total Timesteps: 94788 Episode Num: 210 Episode Len: 1152 Reward: -907.8000000000013 Time Taken: 20.280640602111816 secs\n",
            "Rewards Distribution:  [('road (+1)', 71), ('road (-0.1)', 118), ('sand (-1)', 962), ('wall (-5)', 1)]\n",
            "Training took 48.65863394737244 secs\n",
            "Path [[763, 609], [761, 609], [719, 561], [653, 508], [597, 465], [546, 412], [500, 362], [453, 306], [399, 257], [344, 205], [303, 147], [251, 104], [188, 51]]\n",
            "\n",
            "Total Timesteps: 94899 Episode Num: 211 Episode Len: 111 Reward: -84.20000000000002 Time Taken: 1.9834320545196533 secs\n",
            "Rewards Distribution:  [('road (+1)', 10), ('road (-0.1)', 12), ('sand (-1)', 88), ('wall (-5)', 1)]\n",
            "Training took 4.6867594718933105 secs\n",
            "Path [[899, 91], [900, 90], [854, 23]]\n",
            "\n",
            "Total Timesteps: 95239 Episode Num: 212 Episode Len: 340 Reward: -176.6999999999999 Time Taken: 6.0396435260772705 secs\n",
            "Rewards Distribution:  [('road (+1)', 49), ('road (-0.1)', 77), ('sand (-1)', 213), ('wall (-5)', 1)]\n",
            "Training took 14.363338470458984 secs\n",
            "Path [[824, 179], [823, 180], [782, 132], [735, 86], [675, 39]]\n",
            "\n",
            "Total Timesteps: 95742 Episode Num: 213 Episode Len: 503 Reward: -370.6000000000005 Time Taken: 8.849309206008911 secs\n",
            "Rewards Distribution:  [('road (+1)', 34), ('road (-0.1)', 76), ('sand (-1)', 392), ('wall (-5)', 1)]\n",
            "Training took 21.246419668197632 secs\n",
            "Path [[1159, 293], [1160, 291], [1088, 241], [1042, 184], [996, 131], [951, 81], [889, 21]]\n",
            "\n",
            "Total Timesteps: 96496 Episode Num: 214 Episode Len: 754 Reward: -475.400000000001 Time Taken: 13.288080930709839 secs\n",
            "Rewards Distribution:  [('road (+1)', 90), ('road (-0.1)', 114), ('sand (-1)', 549), ('wall (-5)', 1)]\n",
            "Training took 31.844136476516724 secs\n",
            "Path [[884, 398], [882, 398], [845, 356], [794, 310], [735, 254], [685, 200], [639, 149], [589, 101], [541, 58]]\n",
            "\n",
            "Total Timesteps: 96690 Episode Num: 215 Episode Len: 194 Reward: -181.1 Time Taken: 3.491926431655884 secs\n",
            "Rewards Distribution:  [('road (+1)', 8), ('road (-0.1)', 1), ('sand (-1)', 184), ('wall (-5)', 1)]\n",
            "Training took 8.215797185897827 secs\n",
            "Path [[102, 557], [103, 557], [58, 499]]\n",
            "\n",
            "Total Timesteps: 96933 Episode Num: 216 Episode Len: 243 Reward: -185.39999999999986 Time Taken: 4.269968509674072 secs\n",
            "Rewards Distribution:  [('road (+1)', 20), ('road (-0.1)', 24), ('sand (-1)', 198), ('wall (-5)', 1)]\n",
            "Training took 10.259589910507202 secs\n",
            "Path [[140, 212], [141, 211], [91, 153], [42, 98]]\n",
            "\n",
            "Total Timesteps: 97109 Episode Num: 217 Episode Len: 176 Reward: -131.19999999999987 Time Taken: 3.178886651992798 secs\n",
            "Rewards Distribution:  [('road (+1)', 10), ('road (-0.1)', 32), ('sand (-1)', 133), ('wall (-5)', 1)]\n",
            "Training took 7.43072772026062 secs\n",
            "Path [[1195, 113], [1193, 111], [1134, 67]]\n",
            "\n",
            "Total Timesteps: 98318 Episode Num: 218 Episode Len: 1209 Reward: -913.3000000000013 Time Taken: 21.25923252105713 secs\n",
            "Rewards Distribution:  [('road (+1)', 81), ('road (-0.1)', 153), ('sand (-1)', 974), ('wall (-5)', 1)]\n",
            "Training took 51.05177569389343 secs\n",
            "Path [[804, 622], [805, 621], [750, 563], [698, 511], [649, 470], [604, 434], [538, 382], [480, 330], [437, 275], [389, 229], [326, 189], [280, 125], [219, 72], [173, 27]]\n",
            "\n",
            "Total Timesteps: 98461 Episode Num: 219 Episode Len: 143 Reward: -97.89999999999998 Time Taken: 2.5496037006378174 secs\n",
            "Rewards Distribution:  [('road (+1)', 16), ('road (-0.1)', 19), ('sand (-1)', 107), ('wall (-5)', 1)]\n",
            "Training took 6.042078495025635 secs\n",
            "Path [[339, 89], [337, 88], [294, 47]]\n",
            "\n",
            "Total Timesteps: 98569 Episode Num: 220 Episode Len: 108 Reward: -58.39999999999993 Time Taken: 1.913839340209961 secs\n",
            "Rewards Distribution:  [('road (+1)', 16), ('road (-0.1)', 24), ('sand (-1)', 67), ('wall (-5)', 1)]\n",
            "Training took 4.560912609100342 secs\n",
            "Path [[975, 75], [973, 73], [918, 33]]\n",
            "\n",
            "Total Timesteps: 99189 Episode Num: 221 Episode Len: 620 Reward: -507.60000000000093 Time Taken: 11.026547193527222 secs\n",
            "Rewards Distribution:  [('road (+1)', 33), ('road (-0.1)', 56), ('sand (-1)', 530), ('wall (-5)', 1)]\n",
            "Training took 26.199991941452026 secs\n",
            "Path [[572, 349], [571, 347], [513, 296], [459, 244], [410, 189], [362, 136], [311, 96], [258, 40]]\n",
            "\n",
            "Total Timesteps: 99457 Episode Num: 222 Episode Len: 268 Reward: -147.60000000000002 Time Taken: 4.822310447692871 secs\n",
            "Rewards Distribution:  [('road (+1)', 37), ('road (-0.1)', 56), ('sand (-1)', 174), ('wall (-5)', 1)]\n",
            "Training took 11.310210704803467 secs\n",
            "Path [[745, 161], [745, 162], [701, 101], [648, 49]]\n",
            "\n",
            "Total Timesteps: 99695 Episode Num: 223 Episode Len: 238 Reward: -102.79999999999994 Time Taken: 4.23658561706543 secs\n",
            "Rewards Distribution:  [('road (+1)', 39), ('road (-0.1)', 68), ('sand (-1)', 130), ('wall (-5)', 1)]\n",
            "Training took 10.075333595275879 secs\n",
            "Path [[617, 136], [615, 136], [567, 81], [517, 48]]\n",
            "\n",
            "Total Timesteps: 99891 Episode Num: 224 Episode Len: 196 Reward: -152.39999999999992 Time Taken: 3.486356496810913 secs\n",
            "Rewards Distribution:  [('road (+1)', 13), ('road (-0.1)', 24), ('sand (-1)', 158), ('wall (-5)', 1)]\n",
            "Training took 8.286973714828491 secs\n",
            "Path [[127, 309], [128, 307], [69, 248]]\n",
            "\n",
            "Total Timesteps: 99945 Episode Num: 225 Episode Len: 54 Reward: -37.7 Time Taken: 0.9707045555114746 secs\n",
            "Rewards Distribution:  [('road (+1)', 7), ('road (-0.1)', 7), ('sand (-1)', 39), ('wall (-5)', 1)]\n",
            "Training took 2.269987106323242 secs\n",
            "Path [[845, 53], [845, 54]]\n",
            "\n",
            "Total Timesteps: 100474 Episode Num: 226 Episode Len: 529 Reward: -387.7000000000003 Time Taken: 9.451868534088135 secs\n",
            "Rewards Distribution:  [('road (+1)', 47), ('road (-0.1)', 57), ('sand (-1)', 424), ('wall (-5)', 1)]\n",
            "Training took 22.381150722503662 secs\n",
            "Path [[799, 297], [797, 298], [751, 247], [707, 191], [653, 143], [601, 78], [555, 24]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}